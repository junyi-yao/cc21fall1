[{"path":"index.html","id":"welcome","chapter":"1 Welcome!","heading":"1 Welcome!","text":"ideas can add welcome page? Open issue pull request.","code":""},{"path":"community-contribution.html","id":"community-contribution","chapter":"2 Community Contribution","heading":"2 Community Contribution","text":"fairly open-ended assignment provides opportunity receive credit contributing collective learning class, perhaps beyond. reflect minimum 3 hours work. complete assignment must submit short description contribution. appropriate, attach relevant files.many ways can contribute:organize lead workshop particular topic (date may assignment due date need schedule )help students find final project partnersgive well-rehearsed 5 minute lightning talk class datavis topic (theory tool) (email set date – may assignment due date need schedule )create video tutorial (length)create cheatsheet resourcewrite tutorial tool ’s well documentedbuild viz product (ex. htmlwidget RStudio add-) class use[idea](Note: translations allowed)may draw expand existing resources. , critical cite sources.","code":""},{"path":"community-contribution.html","id":"important-logistics","chapter":"2 Community Contribution","heading":"2.1 IMPORTANT LOGISTICS","text":"","code":""},{"path":"community-contribution.html","id":"groups","chapter":"2 Community Contribution","heading":"2.1.1 Groups","text":"may work partner choosing. work alone, need join group 1, simply submit work CourseWorks solo assignment.work partner, add group CC page People tab. Ed Discussion can used find partners similar interests.","code":""},{"path":"community-contribution.html","id":"what-to-submit","chapter":"2 Community Contribution","heading":"2.1.2 What to submit","text":"cases something tangible upload, tutorial, cheatsheet, etc. Alternatively may submit link material online (YouTube video, etc.) ’s nothing tangible include longer description (see 2.).cases something tangible upload, tutorial, cheatsheet, etc. Alternatively may submit link material online (YouTube video, etc.) ’s nothing tangible include longer description (see 2.).explanation motivation project, need addresses, evaluation project including learned / might differently next time. (1/2 page)explanation motivation project, need addresses, evaluation project including learned / might differently next time. (1/2 page)","code":""},{"path":"community-contribution.html","id":"submitting-your-assignment","chapter":"2 Community Contribution","heading":"2.1.3 Submitting your assignment","text":"must submit assignment twice: CourseWorks (can graded) class, details follow.CourseWorks submission (assignment): submit work .Rmd rendered .pdf .html file, just problem sets. work lend format, write assignment text box .CourseWorks submission (assignment): submit work .Rmd rendered .pdf .html file, just problem sets. work lend format, write assignment text box .Class (GitHub) submission: detail provided separate assignment.Class (GitHub) submission: detail provided separate assignment.","code":""},{"path":"community-contribution.html","id":"grading","chapter":"2 Community Contribution","heading":"2.1.4 Grading","text":"graded quality work, originality, effort invested. sources used must cited.","code":""},{"path":"github-submission-instructions.html","id":"github-submission-instructions","chapter":"3 GitHub submission instructions","heading":"3 GitHub submission instructions","text":"chapter gives information need upload community contribution. Please read entire document carefully making submission. particular note fact bookdown requires different .Rmd format ’re used , must make changes beginning file described submitting.","code":""},{"path":"github-submission-instructions.html","id":"background","chapter":"3 GitHub submission instructions","heading":"3.1 Background","text":"web site makes use bookdown package render collection .Rmd files nicely formatted online book chapters subchapters. job submit slightly modified version community contribution .Rmd file GitHub repository source files web site stored. backend, admins divide chapters book sections order .community contribution different format, create short .Rmd file explains , includes links relevant files, slides, etc. can post GitHub repo (another online site.)","code":""},{"path":"github-submission-instructions.html","id":"preparing-your-.rmd-file","chapter":"3 GitHub submission instructions","heading":"3.2 Preparing your .Rmd file","text":"submit ONE Rmd file.completing modifications, .Rmd look like sample .Rmd.Create concise, descriptive name project. instance, name base_r_ggplot_graph something similar work contrasting/working base R graphics ggplot2 graphics. Check .Rmd filenames file make sure name isn’t already taken. project name words joined underscores, white space. Use .Rmd .rmd. addition, letters must lowercase. Create copy .Rmd file new name.Create concise, descriptive name project. instance, name base_r_ggplot_graph something similar work contrasting/working base R graphics ggplot2 graphics. Check .Rmd filenames file make sure name isn’t already taken. project name words joined underscores, white space. Use .Rmd .rmd. addition, letters must lowercase. Create copy .Rmd file new name.Completely delete YAML header (section top .Rmd includes name, title, date, output, etc.) including --- line.Completely delete YAML header (section top .Rmd includes name, title, date, output, etc.) including --- line.Choose short, descriptive, human readable title project title show table contents – look examples panel left. Capitalize first letter (“sentence case”). first line document, enter single hashtag, followed single whitespace, title. important follow format bookdown renders title header. use single # headers anywhere else document.Choose short, descriptive, human readable title project title show table contents – look examples panel left. Capitalize first letter (“sentence case”). first line document, enter single hashtag, followed single whitespace, title. important follow format bookdown renders title header. use single # headers anywhere else document.second line blank, followed name(s):\n# Base R vs. ggplot2\n\nAaron Burr Alexander Hamilton\n\ncontent starts . second line blank, followed name(s):project requires data, please use built-dataset read directly URL, :\ndf <- readr::read_csv(\"https://people.sc.fsu.edu/~jburkardt/data/csv/addresses.csv\")  absolutely must include data file, please use small one, many reasons desirable keep repository size small possible.project requires data, please use built-dataset read directly URL, :df <- readr::read_csv(\"https://people.sc.fsu.edu/~jburkardt/data/csv/addresses.csv\")  absolutely must include data file, please use small one, many reasons desirable keep repository size small possible.included setup chunk .Rmd file, please remember remove label setup chunk, .e., use:\n{r, include=FALSE}\ninstead :\n{r setup, include=FALSE}included setup chunk .Rmd file, please remember remove label setup chunk, .e., use:instead :project requires libraries installed included document, please adhere following conventions. evaluate install.packages() statements document. Consumers .Rmd file won’t want packages get installed knit document. Include library() statements top .Rmd file, title, name, setup, content. chapter requires installation package source (GitHub installation), please add comment identifying . Please mention well PR. example library() section install statements won’t evaluated:\n\n# remotes::install_github(\"twitter/AnomalyDetection\")\nlibrary(\"AnomalyDetection\") # must installed sourceIf project requires libraries installed included document, please adhere following conventions. evaluate install.packages() statements document. Consumers .Rmd file won’t want packages get installed knit document. Include library() statements top .Rmd file, title, name, setup, content. chapter requires installation package source (GitHub installation), please add comment identifying . Please mention well PR. example library() section install statements won’t evaluated:developed .Rmd file moving library() statements rest file content, highly recommended knit review document . may change namespace available section code development, causing function work exhibit unexpected behavior.file contain getwd() / setwd() calls (never use scripts anyway!) write statements.Want get fancy? See optional tweaks section .","code":"# Base R vs. ggplot2\n\nAaron Burr and Alexander Hamilton\n\nYour content starts here. {r, include=FALSE}{r setup, include=FALSE}\n# remotes::install_github(\"twitter/AnomalyDetection\")\nlibrary(\"AnomalyDetection\") # must be installed from source"},{"path":"github-submission-instructions.html","id":"submission-steps","chapter":"3 GitHub submission instructions","heading":"3.3 Submission steps","text":"submit work, following “Workflow #4” – submitting pull request someone else’s repository write access. Instructions available lecture slides topic well tutorial. repeated abbreviated form, specific instructions naming conventions, content information, important details.Fork cc21fall1 repo (repo) GitHub account.Fork cc21fall1 repo (repo) GitHub account.Clone/download forked repo local computer.Clone/download forked repo local computer.Create new branch name project name, case sample_project. skip step. merge PR doesn’t come branch. already forgot , check tutorial fix .Create new branch name project name, case sample_project. skip step. merge PR doesn’t come branch. already forgot , check tutorial fix .Copy modified .Rmd file name root directory branch. example, sample_project.Rmd.Copy modified .Rmd file name root directory branch. example, sample_project.Rmd.include .html file. (order bookdown package work, .Rmd files rendered behind scenes.)include .html file. (order bookdown package work, .Rmd files rendered behind scenes.)[OPTIONAL] resources (images) included project, create folder resources/. example, resources/sample_project/. Put resources files . sure change links .Rmd include resources/.../, example:\n![Test Photo](resources/sample_project/pumpkins.jpg)[OPTIONAL] resources (images) included project, create folder resources/. example, resources/sample_project/. Put resources files . sure change links .Rmd include resources/.../, example:![Test Photo](resources/sample_project/pumpkins.jpg)ready submit project, push branch remote repo. Follow tutorial create pull request.ready submit project, push branch remote repo. Follow tutorial create pull request.point back forth begin team managing pull requests. asked make changes, simply make changes local branch, save, commit, push GitHub. new commits added pull request; need , , create new pull request. (, based circumstances, make sense close pull request start new one, tell .)point back forth begin team managing pull requests. asked make changes, simply make changes local branch, save, commit, push GitHub. new commits added pull request; need , , create new pull request. (, based circumstances, make sense close pull request start new one, tell .)pull request merged, ’s fine delete local clone (folder) well forked repository GitHub account.pull request merged, ’s fine delete local clone (folder) well forked repository GitHub account.","code":""},{"path":"github-submission-instructions.html","id":"optional-tweaks","chapter":"3 GitHub submission instructions","heading":"3.4 Optional tweaks","text":"prefer links chapter open new tabs, add {target=\"_blank\"} link, :\n[edav.info](edav.info){target=\"_blank\"}prefer links chapter open new tabs, add {target=\"_blank\"} link, :[edav.info](edav.info){target=\"_blank\"}Note headers (##, ###, etc.) converted numbered headings : ## –> 3.1 ### –> 3.1.1  headings appear chapter subheadings sub-subheadings navigation panel left. Think logical structure users navigate chapter. recommend using ## ### headings since “sub-sub-subheadings” 4.1.3.4 generally unnecessary look messy.Note headers (##, ###, etc.) converted numbered headings : ## –> 3.1 ### –> 3.1.1  headings appear chapter subheadings sub-subheadings navigation panel left. Think logical structure users navigate chapter. recommend using ## ### headings since “sub-sub-subheadings” 4.1.3.4 generally unnecessary look messy.Unfortunately, ’s simple way preview chapter ’s actually merged project. (bookdown preview_chapter() option works entire book rendered least become complex require packages project grows.) really want preview , fork clone minimal bookdown repo, add .Rmd file, click “Build book” button Build tab (next Git), open .html files _book folder web browser see rendered book.  ’re interested bookdown options, see official reference book.  useful tweaks share? Submit issue PR.Unfortunately, ’s simple way preview chapter ’s actually merged project. (bookdown preview_chapter() option works entire book rendered least become complex require packages project grows.) really want preview , fork clone minimal bookdown repo, add .Rmd file, click “Build book” button Build tab (next Git), open .html files _book folder web browser see rendered book.  ’re interested bookdown options, see official reference book.  useful tweaks share? Submit issue PR.","code":""},{"path":"github-submission-instructions.html","id":"faq","chapter":"3 GitHub submission instructions","heading":"3.5 FAQ","text":"","code":""},{"path":"github-submission-instructions.html","id":"what-should-i-expect-after-creating-a-pull-request","chapter":"3 GitHub submission instructions","heading":"3.5.1 What should I expect after creating a pull request?","text":"Within week create pull request, apply label assign classmate “PR merger” review files submit see meet requirements.Within week create pull request, apply label assign classmate “PR merger” review files submit see meet requirements.take time can process pull requests, long see pull request repo, don’t worry.take time can process pull requests, long see pull request repo, don’t worry.PR merger contacts regarding pull request, usually means files fail meet requirements. explain wrong, please fix soon possible.PR merger contacts regarding pull request, usually means files fail meet requirements. explain wrong, please fix soon possible.","code":""},{"path":"github-submission-instructions.html","id":"what-if-i-catch-mistakes-before-my-pull-request-is-merged","chapter":"3 GitHub submission instructions","heading":"3.5.2 What if I catch mistakes before my pull request is merged?","text":"Just make changes branch, commit push GitHub. automatically added pull request.","code":""},{"path":"github-submission-instructions.html","id":"what-if-i-catch-mistakes-after-my-pull-request-is-merged","chapter":"3 GitHub submission instructions","heading":"3.5.3 What if I catch mistakes after my pull request is merged?","text":"may submit additional pull requests fix material site. edits small, fixing typos, easiest make edits directly GitHub, following instructions. merge first pull requests edits, please patient.","code":""},{"path":"github-submission-instructions.html","id":"other-questions","chapter":"3 GitHub submission instructions","heading":"3.5.4 Other questions","text":"additional questions, please ask Discussions section respond.Thank contributions!","code":""},{"path":"sample-project.html","id":"sample-project","chapter":"4 Sample project","heading":"4 Sample project","text":"Joe Biden Donald TrumpThis chapter gives sample layout Rmd file.Test Photo","code":""},{"path":"introduction-to-the-lattice-package.html","id":"introduction-to-the-lattice-package","chapter":"5 Introduction to the lattice package","heading":"5 Introduction to the lattice package","text":"Eubin ParkThe lattice package data visualization package created Deepayan Sarkar. add-package improves defaults base R, emphasis displaying multivariate data - supporting creation trellis graphs. strength lattice package mainly ability manage dependent data.general format plotting using lattice functions : graph_type(formula, data). main workhorse function lattice package xyplot().","code":"\nlibrary(lattice)\nlibrary(car)"},{"path":"introduction-to-the-lattice-package.html","id":"producing-a-plot-in-lattice","chapter":"5 Introduction to the lattice package","heading":"5.1 Producing a Plot in Lattice","text":"can begin creating basic plots Lattice - scatterplot. Lattice, done using xyplot. example, use iris dataset.type scatterplot familiar many. seen , can see basic method plotting using xyplot() symbolic formula y ~ x, x independent variable y dependent variable.","code":"\ndata(iris)\nxyplot(Sepal.Length ~ Sepal.Width, data = iris,\n       xlab = \"Sepal Width\",\n       ylab = \"Sepal Length\")"},{"path":"introduction-to-the-lattice-package.html","id":"plotting-by-groups","chapter":"5 Introduction to the lattice package","heading":"5.2 Plotting by Groups","text":"2 main ways going plotting multivariate data lattice.1. Superposition:\ndata plotted region graph, distinct groups able categorized varying plot features color, shapes, etc. use superposition plots, groups argument must specified.2. Juxtaposition:\nData plotted separate regions larger graph. use juxtaposition plots, one must specify conditioning statement, : y ~ x | z, z conditioning variable.difference superposition juxtaposition can shown :Superposition:Juxtaposition:seen , real difference plotting two graphs whether one uses groups argument (Supposition) conditioning statement (Juxtaposition), Lattice able create two different graphs small difference.many problems supposition plot juxtaposition plot overcomes. example, good deal -plotting first plot, result difficult distinguish clear trends within species group. However, problems seen juxtaposition plot.sort advanrage becomes much conspicuous dealing larger multivariate datasets. next example, use quakes dataset.example, created shingles order essentially bin data. shingle contains data subset variable created .example, clear see advantages using juxtaposition method plotting.want re-arrange panels plot, can use layout argument. argument takes vector three values: number rows, number columns, number pages.However, using argument, skewed shapes plots. can fix using aspect argument, controls ratio plots.wanted fit regression lines panel, can use panel function argument xyplot function.","code":"\nxyplot(Sepal.Length ~ Sepal.Width, data=iris, \n       groups=iris$Species, # use groups argument\n       auto.key=list(text=c(\"setosa\", \"versicolor\", \"virginica\")),\n       xlab = \"Sepal Width\",\n       ylab = \"Sepal Length\")\nxyplot(Sepal.Length ~ Sepal.Width | Species, data=iris, # add conditioning statement\n       pch=1, col=\"black\",\n       xlab = \"Sepal Width\",\n       ylab = \"Sepal Length\")\ndata(quakes)\n# Create shingles\nDepth = equal.count(quakes$depth, number = 8, overlap = .1)\n\n# Plot graph using supposition\nxyplot(lat ~ long, data = quakes,\n       groups = Depth,\n       xlab = \"Longitude\",\n       ylab = \"Latitude\")\n# Plot graph using juxtaposition\nxyplot(lat ~ long | Depth, data = quakes,\n       xlab = \"Longtitude\",\n       ylab = \"Latitude\")\n# Use layout argument\nxyplot(lat ~ long | Depth, data = quakes,\n       layout = c(3, 3, 1),\n       xlab = \"Longtitude\",\n       ylab = \"Latitude\")\n# Use aspect argument\nxyplot(lat ~ long | Depth, data = quakes,\n       aspect = 1,\n       layout = c(3, 3, 1),\n       xlab = \"Longtitude\",\n       ylab = \"Latitude\")\n# Use the panel function argument\nxyplot(lat ~ long | Depth, data = quakes,\n       panel = function(x,y,subscripts,...){\n           panel.points(x,y,...)\n           panel.lmline(x,y,...) })"},{"path":"introduction-to-the-lattice-package.html","id":"histograms-and-density-plots","chapter":"5 Introduction to the lattice package","heading":"5.3 Histograms and Density Plots","text":"Lattice offers options , histograms density plots. example, use Duncan dataset car package.make histogram lattice package, use histogram() function.make density plot lattice package, use densityplot() function.can even combine histograms density plots using panel function argument. , can split data separate panels, useful multivariate data.","code":"\ndata(Duncan)\nhistogram(~ prestige, data=Duncan, \n          type=\"count\", # can take 'count', 'percent', or 'density'\n          nint = 10, # number of bins\n          endpoints = c(0, 100)\n          )\ndensityplot(~ prestige, data = Duncan,\n            col = \"black\",\n            plot.points = F # specify whether to have data points\n            )\nb <- with(Duncan, do.breaks(range(income), 3))\n\nxyplot(~income | type, data=Duncan,\n       xlim = range(b), ylim = c(0, 0.04),\n       panel = function(x){\n           panel.histogram(x, \n                           breaks = b, \n                           col=\"gray80\")\n           panel.densityplot(x, \n                             darg =list(n=100),\n                             col=\"red\",\n                             lwd=1.5,\n                             plot.points=F)\n       })"},{"path":"introduction-to-the-lattice-package.html","id":"boxplots-violinplots-and-dotplots","chapter":"5 Introduction to the lattice package","heading":"5.4 Boxplots, Violinplots, and Dotplots","text":"options offered Lattice package include boxplots dotplots. example, use ToothGrowth dataset.make boxplot, use bwplot() function. always lattice package, can use conditioning statement create juxtaposing panels.make violin plot, use bwplot() function specify panel argument.make dotplot, use dotplot() function.","code":"\nbwplot(len ~ supp | dose,  data = ToothGrowth,\n       layout = c(3, 1),\n        xlab = \"Dose\", ylab = \"Length\")\nbwplot(len ~ supp | dose,  data = ToothGrowth,\n       layout = c(3, 1), \n       panel = panel.violin, # specify panel argument to make violin plot\n        xlab = \"Dose\", ylab = \"Length\")\ndotplot(len ~ supp | dose,  data = ToothGrowth,\n       layout = c(3, 1),\n        xlab = \"Dose\", ylab = \"Length\")"},{"path":"introduction-to-the-lattice-package.html","id":"trivariate-plots","chapter":"5 Introduction to the lattice package","heading":"5.5 Trivariate Plots","text":"One option displaying trivariate continuous data utilize 3 axes. can done three-dimensional scatterplot.lattice package, one can create plot using cloud() function. function takes symbolic formula first argument, form: z ~ x * y, x, z, y three continuous variables.example use quakes dataset .view data different perspective, can rotate plot using screen argument. can play feature find best view data.Unfortunately, interactive options available Lattice package.","code":"\ncloud(depth ~ lat * long, data=quakes)\ncloud(depth ~ lat * long, data=quakes,\n      screen = list(z = 105, x = -70))"},{"path":"introduction-to-the-lattice-package.html","id":"pros-and-cons-of-lattice","chapter":"5 Introduction to the lattice package","heading":"5.6 Pros and Cons of Lattice","text":"Now basic overview kinds things lattice package can , let’s discuss advantages disadvantages data visualization package.Pros:good allowing one visualize multivariate data, .e. comparing variable y changes variable x across levels variable zMany settings set automatically entire plot created .Cons:Can difficult flesh entire plot one method callCannot add elements plot created; modified.","code":""},{"path":"graph-animation-with-gganimate-and-after-effects.html","id":"graph-animation-with-gganimate-and-after-effects","chapter":"6 Graph animation with gganimate and After Effects","heading":"6 Graph animation with gganimate and After Effects","text":"Anh-Vu Nguyen\nsite explains graph animation gganimate effects\nlink: https://anhvung.github.io/EDAV-CC/\n","code":""},{"path":"r-vs.-python-visualization-cheatsheet.html","id":"r-vs.-python-visualization-cheatsheet","chapter":"7 R vs. Python Visualization Cheatsheet","heading":"7 R vs. Python Visualization Cheatsheet","text":"Dawei Minhui LiaoWe found primarily work R data visualization may familiar graphs Python. lot differences R Python, usually use ggplot2 R, closest comparable libraries Matplotlib Seaborn working Python. Therefore, made PDF version cheat sheet people primarily use R Python need use one another programming language make plots, easily compare difference graphs.Click following link check cheat sheet:https://github.com/Aaralyn-Liao/EDAV_Contribution_CC8/blob/main/r_vs_python_visualization.pdf","code":""},{"path":"rpackage-waffle-cheatsheet.html","id":"rpackage-waffle-cheatsheet","chapter":"8 Rpackage waffle cheatsheet","heading":"8 Rpackage waffle cheatsheet","text":"Yibo ChenThis project mainly include pdf version cheatsheet R package waffle.Take look cheatsheet using link :\nhttps://github.com/ChenYb9807/cu_edav_cc/blob/main/Community%20Contribution.pdf","code":""},{"path":"r-cheatsheet-on-data-transforamtion-and-exploration.html","id":"r-cheatsheet-on-data-transforamtion-and-exploration","chapter":"9 R cheatsheet on data transforamtion and exploration","heading":"9 R cheatsheet on data transforamtion and exploration","text":"Sai Krupa JangalaFew Pointers:purpose cheatsheet describe basic data operations start new project. also includes different types data transformations, explorations management.use data frames throughtout cheatsheet.use packages commonly used R. using packages different purposes.Different datasets used best illustrate transformation, management exploration.data sets used openintro::fastfood , openintro::seatlepets, cars, openintro::ames mtcars.Every chunk code loads data, ’s , means used data previous chunk.","code":""},{"path":"r-cheatsheet-on-data-transforamtion-and-exploration.html","id":"required-packages","chapter":"9 R cheatsheet on data transforamtion and exploration","heading":"9.0.0.1 Required packages","text":"packages commonly R used following cheatsheet. using packages different purposes.","code":"\n#install.packages(\"tidyverse\")\n#install.packages(\"dplyr\")\n#install.packages(\"reshape\")\nlibrary(dplyr)\nlibrary(tidyverse)\nlibrary(reshape)"},{"path":"r-cheatsheet-on-data-transforamtion-and-exploration.html","id":"output-the-head-tail-and-sample-of-the-dataframe.","chapter":"9 R cheatsheet on data transforamtion and exploration","heading":"9.0.0.2 Output the head, tail and sample of the dataframe.","text":"Head - get first 5 rows dataframe.Tail - get last 5 rows dataframe","code":"\ndata <- cars\nhead(data, n=5)##   speed dist\n## 1     4    2\n## 2     4   10\n## 3     7    4\n## 4     7   22\n## 5     8   16\ntail(data, n=5)##    speed dist\n## 46    24   70\n## 47    24   92\n## 48    24   93\n## 49    24  120\n## 50    25   85"},{"path":"r-cheatsheet-on-data-transforamtion-and-exploration.html","id":"selection-of-only-a-few-columns-from-a-dataframe.","chapter":"9 R cheatsheet on data transforamtion and exploration","heading":"9.0.0.3 Selection of only a few columns from a dataframe.","text":"Select restaurant, item calorie columns fastfood dataset. Two variations shown .can see , transformations produce result.","code":"\ndata <- openintro::fastfood\n# Variation 1\ndata <- data[, c(\"restaurant\",\"item\",\"calories\")]\n# Variation 2\ndata_1 <- select(data, c(restaurant, item, calories))\nhead(data)## # A tibble: 6 × 3\n##   restaurant item                                      calories\n##   <chr>      <chr>                                        <dbl>\n## 1 Mcdonalds  Artisan Grilled Chicken Sandwich               380\n## 2 Mcdonalds  Single Bacon Smokehouse Burger                 840\n## 3 Mcdonalds  Double Bacon Smokehouse Burger                1130\n## 4 Mcdonalds  Grilled Bacon Smokehouse Chicken Sandwich      750\n## 5 Mcdonalds  Crispy Bacon Smokehouse Chicken Sandwich       920\n## 6 Mcdonalds  Big Mac                                        540\nhead(data_1)## # A tibble: 6 × 3\n##   restaurant item                                      calories\n##   <chr>      <chr>                                        <dbl>\n## 1 Mcdonalds  Artisan Grilled Chicken Sandwich               380\n## 2 Mcdonalds  Single Bacon Smokehouse Burger                 840\n## 3 Mcdonalds  Double Bacon Smokehouse Burger                1130\n## 4 Mcdonalds  Grilled Bacon Smokehouse Chicken Sandwich      750\n## 5 Mcdonalds  Crispy Bacon Smokehouse Chicken Sandwich       920\n## 6 Mcdonalds  Big Mac                                        540"},{"path":"r-cheatsheet-on-data-transforamtion-and-exploration.html","id":"get-the-column-names","chapter":"9 R cheatsheet on data transforamtion and exploration","heading":"9.0.0.4 Get the column names","text":"Generate column names data frame","code":"\ndata <- openintro::fastfood\nnames(data)##  [1] \"restaurant\"  \"item\"        \"calories\"    \"cal_fat\"     \"total_fat\"  \n##  [6] \"sat_fat\"     \"trans_fat\"   \"cholesterol\" \"sodium\"      \"total_carb\" \n## [11] \"fiber\"       \"sugar\"       \"protein\"     \"vit_a\"       \"vit_c\"      \n## [16] \"calcium\"     \"salad\""},{"path":"r-cheatsheet-on-data-transforamtion-and-exploration.html","id":"drop-one-or-more-columns.","chapter":"9 R cheatsheet on data transforamtion and exploration","heading":"9.0.0.5 Drop one or more columns.","text":"Drop columns restaurant, item, calories data. can observe, column names dropped.","code":"\ndata <- openintro::fastfood\ndata <- select(data, -c(restaurant, item, calories))\nnames(data)##  [1] \"cal_fat\"     \"total_fat\"   \"sat_fat\"     \"trans_fat\"   \"cholesterol\"\n##  [6] \"sodium\"      \"total_carb\"  \"fiber\"       \"sugar\"       \"protein\"    \n## [11] \"vit_a\"       \"vit_c\"       \"calcium\"     \"salad\""},{"path":"r-cheatsheet-on-data-transforamtion-and-exploration.html","id":"transformation-using-the-transform-function-in-r","chapter":"9 R cheatsheet on data transforamtion and exploration","heading":"9.0.0.6 Transformation using the transform() function in R","text":"creating new dataframe changing speed column. transformed multiplying 100Here transformed original dataframe creating new column.","code":"\ndata <- cars\ndata_1 <- transform(data, speed=speed*100)\nhead(data_1)##   speed dist\n## 1   400    2\n## 2   400   10\n## 3   700    4\n## 4   700   22\n## 5   800   16\n## 6   900   10\ndata <- transform(data, time=speed*dist)\nhead(data)##   speed dist time\n## 1     4    2    8\n## 2     4   10   40\n## 3     7    4   28\n## 4     7   22  154\n## 5     8   16  128\n## 6     9   10   90"},{"path":"r-cheatsheet-on-data-transforamtion-and-exploration.html","id":"conditional-transformation-in-r","chapter":"9 R cheatsheet on data transforamtion and exploration","heading":"9.0.0.7 Conditional Transformation in R","text":"Transformation based condition. creating new column called Grilled, assigned Grilled item contains Grilled ’s name else classified Grilled.","code":"\n#Check if it contains the word grilled.\ndata <- openintro::fastfood\ndata <- transform(data, Grilled=ifelse(str_detect(item, \"Grilled\"), \"Grilled\", \"Not Grilled\"))\nhead(data[,c(\"item\",\"Grilled\")])##                                        item     Grilled\n## 1          Artisan Grilled Chicken Sandwich     Grilled\n## 2            Single Bacon Smokehouse Burger Not Grilled\n## 3            Double Bacon Smokehouse Burger Not Grilled\n## 4 Grilled Bacon Smokehouse Chicken Sandwich     Grilled\n## 5  Crispy Bacon Smokehouse Chicken Sandwich Not Grilled\n## 6                                   Big Mac Not Grilled"},{"path":"r-cheatsheet-on-data-transforamtion-and-exploration.html","id":"add-a-new-column-to-the-dataframe-without-transform-function.","chapter":"9 R cheatsheet on data transforamtion and exploration","heading":"9.0.0.8 Add a new column to the dataframe without transform() function.","text":"added new column called time.","code":"\ndata <- cars\ndata$time <- data$speed * data$dist\nhead(data)##   speed dist time\n## 1     4    2    8\n## 2     4   10   40\n## 3     7    4   28\n## 4     7   22  154\n## 5     8   16  128\n## 6     9   10   90"},{"path":"r-cheatsheet-on-data-transforamtion-and-exploration.html","id":"get-all-the-unique-values-of-a-column-in-a-dataframe.","chapter":"9 R cheatsheet on data transforamtion and exploration","heading":"9.0.0.9 Get all the unique values of a column in a dataframe.","text":"","code":"\ndata <- openintro::fastfood\nunique(data$restaurant)## [1] \"Mcdonalds\"   \"Chick Fil-A\" \"Sonic\"       \"Arbys\"       \"Burger King\"\n## [6] \"Dairy Queen\" \"Subway\"      \"Taco Bell\""},{"path":"r-cheatsheet-on-data-transforamtion-and-exploration.html","id":"using-the-filter-function-in-r.-different-logical-operators-can-be-used-to-filter-the-data.","chapter":"9 R cheatsheet on data transforamtion and exploration","heading":"9.0.0.10 Using the filter function in R. Different logical operators can be used to filter the data.","text":"Filter rows mpg column value 21.0Filter rows mpg column value less 21.0Filter rows mpg column value greater 21.0Filter rows cyl column 4 carb column greater 1.","code":"\ndata <- mtcars\ndata_filtered <- filter(data, mpg==21.0)\nunique(data_filtered$mpg)## [1] 21\ndata_filtered_1 <- filter(data, mpg<21.0)\nunique(data_filtered_1$mpg)##  [1] 18.7 18.1 14.3 19.2 17.8 16.4 17.3 15.2 10.4 14.7 15.5 13.3 15.8 19.7 15.0\ndata_filtered_2 <- filter(data, mpg>21.0)\nunique(data_filtered_2$mpg)## [1] 22.8 21.4 24.4 32.4 30.4 33.9 21.5 27.3 26.0\ndata_filtered_logical <- filter(data, cyl == 4 & carb > 1)\nunique(data_filtered_logical$mpg)## [1] 24.4 22.8 30.4 26.0 21.4"},{"path":"r-cheatsheet-on-data-transforamtion-and-exploration.html","id":"select-only-few-rows-in-a-column-based-on-a-condition-without-using-the-filter-function.","chapter":"9 R cheatsheet on data transforamtion and exploration","heading":"9.0.0.11 Select only few rows in a column based on a condition without using the filter() function.","text":"selecting rows restuarant name subway","code":"\ndata <- openintro::fastfood\n# Select the rows where the item contains the word \"Grilled\ndata <- data[data$restaurant == \"Subway\", ] \nunique(data$restaurant)## [1] \"Subway\""},{"path":"r-cheatsheet-on-data-transforamtion-and-exploration.html","id":"merge-two-dataframes","chapter":"9 R cheatsheet on data transforamtion and exploration","heading":"9.0.0.12 Merge two dataframes","text":"Merging two dataframes based column names. Authors dataframe books dataframe merged surname name.","code":"\nauthors <- data.frame(\n    surname = c(\"Tukey\", \"Venables\", \"Tierney\", \"Ripley\", \"McNeil\"),\n    nationality = c(\"US\", \"Australia\", \"US\", \"UK\", \"Australia\"),\n    retired = c(\"yes\", rep(\"no\", 4)))\nbooks <- data.frame(\n    name = c(\"Tukey\", \"Venables\", \"Tierney\", \"Ripley\", \"Ripley\", \"McNeil\"),\n    title = c(\"Exploratory Data Analysis\",\n              \"Probability and Statistics\",\n              \"Finance and Structuring for Data Science\",\n              \"Algorithms for Data Science\",\n               \"Interactive Data Analysis\",\n              \"Deep Learning\"))\n    #other.author = c(NA, \"Ripley\", NA, NA, NA, NA))\nmerged <- merge(authors, books, by.x=\"surname\", by.y=\"name\")\nhead(merged)##    surname nationality retired                                    title\n## 1   McNeil   Australia      no                            Deep Learning\n## 2   Ripley          UK      no              Algorithms for Data Science\n## 3   Ripley          UK      no                Interactive Data Analysis\n## 4  Tierney          US      no Finance and Structuring for Data Science\n## 5    Tukey          US     yes                Exploratory Data Analysis\n## 6 Venables   Australia      no               Probability and Statistics"},{"path":"r-cheatsheet-on-data-transforamtion-and-exploration.html","id":"arrange-the-data-in-ascending-order-based-on-a-column","chapter":"9 R cheatsheet on data transforamtion and exploration","heading":"9.0.0.13 Arrange the data in ascending order based on a column","text":"","code":"\ndata <- openintro::fastfood\n# Arranging the data in ascending order\ndata <- data[order(data$total_fat),] \nhead(data[, c(\"restaurant\",\"total_fat\")])## # A tibble: 6 × 2\n##   restaurant  total_fat\n##   <chr>           <dbl>\n## 1 Dairy Queen         0\n## 2 Subway              1\n## 3 Chick Fil-A         2\n## 4 Dairy Queen         2\n## 5 Subway              2\n## 6 Subway              2"},{"path":"r-cheatsheet-on-data-transforamtion-and-exploration.html","id":"arrange-the-data-in-descending-order-based-on-a-column","chapter":"9 R cheatsheet on data transforamtion and exploration","heading":"9.0.0.14 Arrange the data in descending order based on a column","text":"","code":"\n# Arranging the data in descending order\ndata <- data[order(data$total_fat, decreasing = TRUE),]\nhead(data[, c(\"restaurant\",\"total_fat\")])## # A tibble: 6 × 2\n##   restaurant  total_fat\n##   <chr>           <dbl>\n## 1 Mcdonalds         141\n## 2 Burger King       126\n## 3 Mcdonalds         107\n## 4 Sonic             100\n## 5 Sonic              92\n## 6 Mcdonalds          88"},{"path":"r-cheatsheet-on-data-transforamtion-and-exploration.html","id":"get-the-summary-of-a-column---mean-median-var-sd-etc","chapter":"9 R cheatsheet on data transforamtion and exploration","heading":"9.0.0.15 Get the summary of a column -> mean, median, var, SD etc","text":"function gives us Minimum value, 1st Quartile value, Median, Mean, 3rd Quartile value, Maximum value column data frame.","code":"\nsummary(data$total_fat)##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##    0.00   14.00   23.00   26.59   35.00  141.00"},{"path":"r-cheatsheet-on-data-transforamtion-and-exploration.html","id":"convert-all-the-values-in-the-column-to-upper-case.","chapter":"9 R cheatsheet on data transforamtion and exploration","heading":"9.0.0.16 Convert all the values in the column to upper case.","text":"","code":"\ndata$item <- tolower(data$item)\nhead(data$item)## [1] \"20 piece buttermilk crispy chicken tenders\"      \n## [2] \"american brewhouse king\"                         \n## [3] \"40 piece chicken mcnuggets\"                      \n## [4] \"garlic parmesan dunked ultimate chicken sandwich\"\n## [5] \"super sonic bacon double cheeseburger (w/mayo)\"  \n## [6] \"12 piece buttermilk crispy chicken tenders\""},{"path":"r-cheatsheet-on-data-transforamtion-and-exploration.html","id":"convert-all-the-values-in-the-column-to-lower-case.","chapter":"9 R cheatsheet on data transforamtion and exploration","heading":"9.0.0.17 Convert all the values in the column to lower case.","text":"","code":"\ndata$item <- toupper(data$item)\nhead(data$item)## [1] \"20 PIECE BUTTERMILK CRISPY CHICKEN TENDERS\"      \n## [2] \"AMERICAN BREWHOUSE KING\"                         \n## [3] \"40 PIECE CHICKEN MCNUGGETS\"                      \n## [4] \"GARLIC PARMESAN DUNKED ULTIMATE CHICKEN SANDWICH\"\n## [5] \"SUPER SONIC BACON DOUBLE CHEESEBURGER (W/MAYO)\"  \n## [6] \"12 PIECE BUTTERMILK CRISPY CHICKEN TENDERS\""},{"path":"r-cheatsheet-on-data-transforamtion-and-exploration.html","id":"dropping-nas","chapter":"9 R cheatsheet on data transforamtion and exploration","heading":"9.0.0.18 Dropping NAs","text":"Dropping rows one columns NA","code":"\ndata <- openintro::seattlepets\ndata <- data[complete.cases(data), ]\n# Removes the rows with one or more columns having a NA"},{"path":"r-cheatsheet-on-data-transforamtion-and-exploration.html","id":"groupby-and-summarize-functions-usage","chapter":"9 R cheatsheet on data transforamtion and exploration","heading":"9.0.0.19 groupby( ) and summarize( ) functions usage","text":"use ames data demonstrate functions. want find minimum, maximum area houses particular neighborhood, group Neighborhood compute minimum maximum area columns using summarise function.","code":"\ndata <- openintro::ames\ndata <- data %>% group_by(Neighborhood) %>% summarise(max_area= max(area), min_area=min(area))\nhead(data, n=10)## # A tibble: 10 × 3\n##    Neighborhood max_area min_area\n##    <fct>           <int>    <int>\n##  1 Blmngtn          1589     1142\n##  2 Blueste          1556     1020\n##  3 BrDale           1365      948\n##  4 BrkSide          2134      334\n##  5 ClearCr          3086      988\n##  6 CollgCr          2828      768\n##  7 Crawfor          3447      694\n##  8 Edwards          5642      498\n##  9 Gilbert          2462      864\n## 10 Greens           1295      788"},{"path":"r-cheatsheet-on-data-transforamtion-and-exploration.html","id":"get-the-shape-of-the-dataframe-in-r.","chapter":"9 R cheatsheet on data transforamtion and exploration","heading":"9.0.0.20 Get the shape of the dataframe in R.","text":"know number rows columns dataframe.First number list number rows second number list number columns dataframe","code":"\ndata <- openintro::ames\ndim(data)## [1] 2930   82"},{"path":"r-cheatsheet-on-data-transforamtion-and-exploration.html","id":"select-only-top-30-rows-or-90-rows-bottom-30-rows","chapter":"9 R cheatsheet on data transforamtion and exploration","heading":"9.0.0.21 Select only top 30 rows or 90 rows, bottom 30 rows","text":"","code":"\ndata <- openintro::ames\n# Selecting the top 30 rows\ndata <- data[1:30,]\n# Selecting the top 90 rows\ndata <- data[1:90,]\ndata <- openintro::ames\ndata <- data[2900:2930,] \n# Select the last 30 rows if your dataframe consists of 2930 rows"},{"path":"r-cheatsheet-on-data-transforamtion-and-exploration.html","id":"get-the-data-type-of-each-column-in-a-dataframe.","chapter":"9 R cheatsheet on data transforamtion and exploration","heading":"9.0.0.22 Get the data type of each column in a dataframe.","text":"","code":"\ndata <- openintro::seattlepets\nmap(data, class)## $license_issue_date\n## [1] \"Date\"\n## \n## $license_number\n## [1] \"character\"\n## \n## $animal_name\n## [1] \"character\"\n## \n## $species\n## [1] \"character\"\n## \n## $primary_breed\n## [1] \"character\"\n## \n## $secondary_breed\n## [1] \"character\"\n## \n## $zip_code\n## [1] \"character\""},{"path":"r-cheatsheet-on-data-transforamtion-and-exploration.html","id":"changing-the-type-of-data---int-to-char","chapter":"9 R cheatsheet on data transforamtion and exploration","heading":"9.0.0.23 Changing the type of data -> int to char","text":"Change values column character integer valid values.Note: rows may marked NA coercion.","code":"\ndata <- openintro::seattlepets\nhead(data$zip_code)## [1] \"98108\" \"98117\" \"98136\" \"98117\" \"98144\" \"98103\"\ndata$zip_code <- as.numeric(data$zip_code)\nhead(data$zip_code)## [1] 98108 98117 98136 98117 98144 98103"},{"path":"r-cheatsheet-on-data-transforamtion-and-exploration.html","id":"rename-column-names-in-r","chapter":"9 R cheatsheet on data transforamtion and exploration","heading":"9.0.0.24 Rename column names in R","text":"renaming column name zip_code zip_code_modified","code":"\nnames(data)[names(data) == 'zip_code'] <- 'zip_code_modified'\nnames(data)## [1] \"license_issue_date\" \"license_number\"     \"animal_name\"       \n## [4] \"species\"            \"primary_breed\"      \"secondary_breed\"   \n## [7] \"zip_code_modified\"\n# Zip code has been modified to zip_code_modified"},{"path":"r-cheatsheet-on-data-transforamtion-and-exploration.html","id":"find-minimum-and-maximum-values-in-a-column-in-r","chapter":"9 R cheatsheet on data transforamtion and exploration","heading":"9.0.0.25 Find minimum and maximum values in a column in R","text":"Minimum value area columnMaximum value area column","code":"\ndata <- openintro::ames\nmin <- min(data$area)\nmax <- max(data$area)\nmin## [1] 334\nmax## [1] 5642"},{"path":"r-cheatsheet-on-data-transforamtion-and-exploration.html","id":"number-of-unique-values-in-every-column-in-the-data-frame","chapter":"9 R cheatsheet on data transforamtion and exploration","heading":"9.0.0.26 Number of unique values in every column in the data frame","text":"dataset, 13930 unique animal names, 4 different species etc.","code":"\ndata <- openintro::seattlepets\ndata %>% summarize_all(n_distinct)## # A tibble: 1 × 7\n##   license_issue_date license_number animal_name species primary_breed\n##                <int>          <int>       <int>   <int>         <int>\n## 1               1064          52497       13930       4           336\n## # … with 2 more variables: secondary_breed <int>, zip_code <int>"},{"path":"r-cheatsheet-on-data-transforamtion-and-exploration.html","id":"reorder-columns-in-r-by-column-name.","chapter":"9 R cheatsheet on data transforamtion and exploration","heading":"9.0.0.27 Reorder columns in R by column name.","text":"","code":"\nauthors <- data.frame(\n    surname = c(\"Tukey\", \"Venables\", \"Tierney\", \"Ripley\", \"McNeil\"),\n    nationality = c(\"US\", \"Australia\", \"US\", \"UK\", \"Australia\"),\n    retired = c(\"yes\", rep(\"no\", 4)))\nauthors##    surname nationality retired\n## 1    Tukey          US     yes\n## 2 Venables   Australia      no\n## 3  Tierney          US      no\n## 4   Ripley          UK      no\n## 5   McNeil   Australia      no\n#reorder by column name\nauthors <- authors[, c(\"retired\", \"nationality\", \"surname\")]\nauthors##   retired nationality  surname\n## 1     yes          US    Tukey\n## 2      no   Australia Venables\n## 3      no          US  Tierney\n## 4      no          UK   Ripley\n## 5      no   Australia   McNeil"},{"path":"r-cheatsheet-on-data-transforamtion-and-exploration.html","id":"reorder-columns-by-column-index.","chapter":"9 R cheatsheet on data transforamtion and exploration","heading":"9.0.0.28 Reorder columns by column index.","text":"","code":"\nauthors <- authors[, c(1,3,2)]\nauthors##   retired  surname nationality\n## 1     yes    Tukey          US\n## 2      no Venables   Australia\n## 3      no  Tierney          US\n## 4      no   Ripley          UK\n## 5      no   McNeil   Australia"},{"path":"r-cheatsheet-on-data-transforamtion-and-exploration.html","id":"remove-duplicates-from-the-dataframe-based-on-one-column-or-multiple-columns.","chapter":"9 R cheatsheet on data transforamtion and exploration","heading":"9.0.0.29 Remove Duplicates from the dataframe based on one column or multiple columns.","text":"","code":"\n# Remove the duplicate rows based on one variable\ndata <- mtcars\n# Have the rows with distinct carb\ndata_one_var <- distinct(data, carb, .keep_all= TRUE)\n# Keep the distinct data based on multiple variables\ndata_multi <- distinct(data, cyl, vs, .keep_all= TRUE)"},{"path":"r-cheatsheet-on-data-transforamtion-and-exploration.html","id":"calculate-mean-median-of-a-column-in-the-data-frame","chapter":"9 R cheatsheet on data transforamtion and exploration","heading":"9.0.0.30 Calculate Mean, Median of a column in the Data Frame","text":"MeanMedian","code":"\nmean <- mean(data$carb)\nmedian <- median(data$carb)\nmean## [1] 2.8125\nmedian## [1] 2"},{"path":"r-cheatsheet-on-data-transforamtion-and-exploration.html","id":"value-counts-in-r.-check-how-many-times-a-unique-variable-occurs-in-like-male---5-female--10-in-the-column-name-gender.","chapter":"9 R cheatsheet on data transforamtion and exploration","heading":"9.0.0.31 Value counts in R. Check how many times a unique variable occurs in like Male - 5, Female -10 in the column name Gender.","text":"Number rows cyl = 4 11, cyl=6 7 etcNumber rows carb = 4 10, carb=6 1 etc","code":"\ndata %>% count(cyl)##   cyl  n\n## 1   4 11\n## 2   6  7\n## 3   8 14\ndata %>% count(carb)##   carb  n\n## 1    1  7\n## 2    2 10\n## 3    3  3\n## 4    4 10\n## 5    6  1\n## 6    8  1"},{"path":"r-cheatsheet-on-data-transforamtion-and-exploration.html","id":"convert-the-index-column-to-a-new-column","chapter":"9 R cheatsheet on data transforamtion and exploration","heading":"9.0.0.32 Convert the index column to a new column","text":"created new column using index dataframe.","code":"\ndata <- cbind(car_name = rownames(data), data)\nhead(data)##                            car_name  mpg cyl disp  hp drat    wt  qsec vs am\n## Mazda RX4                 Mazda RX4 21.0   6  160 110 3.90 2.620 16.46  0  1\n## Mazda RX4 Wag         Mazda RX4 Wag 21.0   6  160 110 3.90 2.875 17.02  0  1\n## Datsun 710               Datsun 710 22.8   4  108  93 3.85 2.320 18.61  1  1\n## Hornet 4 Drive       Hornet 4 Drive 21.4   6  258 110 3.08 3.215 19.44  1  0\n## Hornet Sportabout Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0\n## Valiant                     Valiant 18.1   6  225 105 2.76 3.460 20.22  1  0\n##                   gear carb\n## Mazda RX4            4    4\n## Mazda RX4 Wag        4    4\n## Datsun 710           4    1\n## Hornet 4 Drive       3    1\n## Hornet Sportabout    3    2\n## Valiant              3    1"},{"path":"r-cheatsheet-on-data-transforamtion-and-exploration.html","id":"add-a-new-row-to-the-dataframe","chapter":"9 R cheatsheet on data transforamtion and exploration","heading":"9.0.0.33 Add a new row to the dataframe","text":"can new rows dataframe using rbind","code":"\nnew_row_to_add <- data.frame(\"Volvo 125\",22.5,3,120.2,108,2.23,2.89,19.08,1,0,4,3)\nnames(new_row_to_add) <- c(\"car_name\", \"mpg\", \"cyl\", \"disp\", \"hp\", \"drat\", \"wt\", \"qsec\", \"vs\", \"am\", \"gear\", \"carb\")\ndata <- rbind(data,new_row_to_add)\ntail(data)##                      car_name  mpg cyl  disp  hp drat    wt  qsec vs am gear\n## Lotus Europa     Lotus Europa 30.4   4  95.1 113 3.77 1.513 16.90  1  1    5\n## Ford Pantera L Ford Pantera L 15.8   8 351.0 264 4.22 3.170 14.50  0  1    5\n## Ferrari Dino     Ferrari Dino 19.7   6 145.0 175 3.62 2.770 15.50  0  1    5\n## Maserati Bora   Maserati Bora 15.0   8 301.0 335 3.54 3.570 14.60  0  1    5\n## Volvo 142E         Volvo 142E 21.4   4 121.0 109 4.11 2.780 18.60  1  1    4\n## 1                   Volvo 125 22.5   3 120.2 108 2.23 2.890 19.08  1  0    4\n##                carb\n## Lotus Europa      2\n## Ford Pantera L    4\n## Ferrari Dino      6\n## Maserati Bora     8\n## Volvo 142E        2\n## 1                 3"},{"path":"r-cheatsheet-on-data-transforamtion-and-exploration.html","id":"logsquare-root-cube-root-transformation-in-r","chapter":"9 R cheatsheet on data transforamtion and exploration","heading":"9.0.0.34 Log,Square root, Cube root transformation in R","text":"","code":"\ndata <- cars\n# Taking the log of the speed column\ndata$log_transformation <- log10(data$speed)\n# Taking the square root of the speed column\ndata$sqrt_transformation <- sqrt(data$speed)\n# Taking the cube root of the speed column\ndata$cube_transformation <-(data$speed)^1/3\nhead(data)##   speed dist log_transformation sqrt_transformation cube_transformation\n## 1     4    2          0.6020600            2.000000            1.333333\n## 2     4   10          0.6020600            2.000000            1.333333\n## 3     7    4          0.8450980            2.645751            2.333333\n## 4     7   22          0.8450980            2.645751            2.333333\n## 5     8   16          0.9030900            2.828427            2.666667\n## 6     9   10          0.9542425            3.000000            3.000000"},{"path":"r-cheatsheet-on-data-transforamtion-and-exploration.html","id":"changing-the-dataframe-dimensions-from-wide-to-long","chapter":"9 R cheatsheet on data transforamtion and exploration","heading":"9.0.0.35 Changing the dataframe dimensions from wide to long","text":"following types can use melt() function R.Type 1:\ncreate two columns called variable - represent subject value represents grade subject.Type 2:\ninterested grade english math, can pass measure.vars parameter.","code":"\ndf_wide <- data.frame(\n  student = c(\"Krupa\", \"Goutham\", \"Sailaja\", \"Murthy\"),\n  school = c(\"St. Joseph's\", \"Timpany\", \"St.Joseph's\", \"Timpany\"),\n  exploratory_data_analysis = c(10, 100, 1000, 10000),  # eng grades\n  probability_and_statistics = c(20, 200, 2000, 20000),  # math grades\n  algorithms_for_ds = c(30, 300, 3000, 30000)   # physics grades\n)\ndf_long <- melt(data = df_wide, \n                id.vars = c(\"student\", \"school\"))\ndf_long##    student       school                   variable value\n## 1    Krupa St. Joseph's  exploratory_data_analysis    10\n## 2  Goutham      Timpany  exploratory_data_analysis   100\n## 3  Sailaja  St.Joseph's  exploratory_data_analysis  1000\n## 4   Murthy      Timpany  exploratory_data_analysis 10000\n## 5    Krupa St. Joseph's probability_and_statistics    20\n## 6  Goutham      Timpany probability_and_statistics   200\n## 7  Sailaja  St.Joseph's probability_and_statistics  2000\n## 8   Murthy      Timpany probability_and_statistics 20000\n## 9    Krupa St. Joseph's          algorithms_for_ds    30\n## 10 Goutham      Timpany          algorithms_for_ds   300\n## 11 Sailaja  St.Joseph's          algorithms_for_ds  3000\n## 12  Murthy      Timpany          algorithms_for_ds 30000\ndf_long <- melt(data = df_wide, \n                id.vars = \"student\",\n                measure.vars = c(\"exploratory_data_analysis\", \"algorithms_for_ds\"))\ndf_long##   student                  variable value\n## 1   Krupa exploratory_data_analysis    10\n## 2 Goutham exploratory_data_analysis   100\n## 3 Sailaja exploratory_data_analysis  1000\n## 4  Murthy exploratory_data_analysis 10000\n## 5   Krupa         algorithms_for_ds    30\n## 6 Goutham         algorithms_for_ds   300\n## 7 Sailaja         algorithms_for_ds  3000\n## 8  Murthy         algorithms_for_ds 30000"},{"path":"r-cheatsheet-on-data-transforamtion-and-exploration.html","id":"replace-na-with-a-specific-value","chapter":"9 R cheatsheet on data transforamtion and exploration","heading":"9.0.0.36 Replace NA with a specific value","text":"replacing NA None.References:\n* https://towardsdatascience.com/data-cleaning--r-made-simple-1b77303b0b17\n* https://towardsdatascience.com/data-transformation--r-288e95438ff9\n* https://bookdown.org/aschmi11/RESMHandbook/data-preparation--cleaning--r.html","code":"\ndata <- openintro::seattlepets\ndata[is.na(data)] <- \"None\""},{"path":"r-vs-python-basic-data-wrangling.html","id":"r-vs-python-basic-data-wrangling","chapter":"10 R vs Python Basic Data Wrangling","heading":"10 R vs Python Basic Data Wrangling","text":"Yihao GaoThis pdf syntax comparison cheat sheet basic data wrangling R (dplyr, tidyr) Python (pandas) including row/column selection, missing value imputation, sorting, mapping, grouping aggregation etc. motivation creating cheat sheet give quick reference might sometimes mix syntax Python R save time.Access cheat sheet using link .\nhttps://github.com/yiiihao/yg2820_edav_CC/blob/main/cheetsheet.pdf","code":""},{"path":"tidyverse_cheatsheet.html","id":"tidyverse_cheatsheet","chapter":"11 tidyverse_cheatsheet","heading":"11 tidyverse_cheatsheet","text":"Ziyu SongMotivation:R one widely used programming languages today’s society. R can help people build statistical models also visualize data analysis results. However, many R packages functions users may feel hard remember details apply data analysis process. project, create cheat sheet Tidyverse, powerful collection R packages data tools transforming visualizing data, help users quickly find information need want exploratory data visualization R.cheat sheet includes main knowledge points popular functions Tidyverse, including definition Tidyverse, core Tidyverse packages, typical data analysis workflow using Tidyverse, key functions core packages. order make easier users understand function provided, incorporated example axis parameters.Self-Evaluation:creating cheat sheet summarizing main knowledge points Tidyverse, gained better understanding main packages Tidyverse consolidated knowledge learned class. found Tidyverse provides efficient, fast, well-documented workflow general data modeling visualization tasks. hand, many important useful packages Tidyverse. Therefore, hope create complete cheat sheet Tidyverse next time help people better understand apply data analysis process.Cheatsheet:\nhttps://github.com/zs2488/cc21fall1/blob/main/Community%20Contribution%20-%20Cheat%20Sheet.pdf","code":""},{"path":"cheatsheet-tidyverse.html","id":"cheatsheet-tidyverse","chapter":"12 Cheatsheet Tidyverse","heading":"12 Cheatsheet Tidyverse","text":"Jiazhen Li","code":""},{"path":"cheatsheet-tidyverse.html","id":"section","chapter":"12 Cheatsheet Tidyverse","heading":"12.1 ","text":"cheatsheet helping students start learn analysis graph R. class, learn tidyverse, powerful collection R packages. cheatsheet, pay attention two packages: ggplot2 dplyr. specifically list functions dylyr frequently use grammer graphics ggplot2 packages. useful new student learn.Cheatsheet Tidyverse: https://github.com/Jiazhen980326/contribution-/blob/master/cheatsheet-Jiazhen%20Li%20.pdf","code":""},{"path":"multithreading-crawler-frame.html","id":"multithreading-crawler-frame","chapter":"13 Multithreading Crawler Frame","heading":"13 Multithreading Crawler Frame","text":"Yi YangAll work uploaded Github, ’s url:https://github.com/yiyangnju/multithreading-crawler-frame","code":""},{"path":"regression-and-classification-in-r.html","id":"regression-and-classification-in-r","chapter":"14 Regression and Classification in R","heading":"14 Regression and Classification in R","text":"Parv JoshiThis video tutorial, can found https://youtu./J2rnDy9PB3E. code created part video given contents file, reference. links used data set:Data.csvData.csvTitanic.csvTitanic.csvAmes_Housing_data.csvAmes_Housing_data.csvBoxcox Implementation R - 1Boxcox Implementation R - 1Boxcox Implementation R - 2Boxcox Implementation R - 2Peanalized RegressionPeanalized RegressionStepwise Selection MethodStepwise Selection MethodAccuracy MetricsAccuracy MetricsRmd CheatsheetRmd Cheatsheet","code":""},{"path":"regression-and-classification-in-r.html","id":"libraries-and-warnings","chapter":"14 Regression and Classification in R","heading":"14.0.1 Libraries and Warnings","text":"","code":"\n# Removing messages and warnings from knited version\nknitr::opts_chunk$set(warning = FALSE, message = FALSE)\n\n# Libraries\n# Make sure these are installed before running them. They all are a part of CRAN.\n\nlibrary(RCurl)\nlibrary(tidyverse)\nlibrary(randomForest)\nlibrary(caTools)\nlibrary(car)\nlibrary(MASS)\nlibrary(leaps)\nlibrary(caret)\nlibrary(bestglm)\nlibrary(rpart)\nlibrary(rattle)"},{"path":"regression-and-classification-in-r.html","id":"reading-data","chapter":"14 Regression and Classification in R","heading":"14.0.2 Reading Data","text":"","code":"\n# Importing the dataset\n\ndataset = read.csv(\"https://raw.githubusercontent.com/Parv-Joshi/EDAV_CC_Datasets/main/Data.csv\")\n\n# str(dataset)\n# View(dataset)"},{"path":"regression-and-classification-in-r.html","id":"data-preprocessing","chapter":"14 Regression and Classification in R","heading":"14.0.3 Data Preprocessing","text":"","code":"\n# Mean Imputation for Missing Data\ndataset$Age = ifelse(is.na(dataset$Age),\n                     ave(dataset$Age, FUN = function(x) mean(x, na.rm = T)),\n                     dataset$Age)\n\ndataset$Salary = ifelse(is.na(dataset$Salary),\n                        ave(dataset$Salary, FUN = function(x) mean(x, na.rm = T)),\n                        dataset$Salary)\n\n# Encoding Categorical Variables\ndataset$Country = factor(dataset$Country, \n                         labels = c(\"France\", \"Spain\", \"Germany\"), \n                         levels = c(\"France\", \"Spain\", \"Germany\"))\ndataset$Purchased = factor(dataset$Purchased, \n                           levels = c(\"Yes\", \"No\"), \n                           labels = c(1, 0))\n\n# Splitting Data into Training and Testing\n\nset.seed(123)\n\nsplit = sample.split(dataset$Purchased, SplitRatio = 0.8)\ntraining_set = subset(dataset, split == T)\ntest_set = subset(dataset, split == F)\n\n# Feature Scaling\ntraining_set[, 2:3] = scale(training_set[, 2:3])\ntest_set[, 2:3] = scale(test_set[, 2:3])"},{"path":"regression-and-classification-in-r.html","id":"regression","chapter":"14 Regression and Classification in R","heading":"14.0.4 Regression","text":"","code":"\n# Data \n\ndata(\"Salaries\", package = \"carData\")\n# force(Salaries)\n\nattach(Salaries)\ndetach(Salaries)\n\n# str(Salaries)\n# View(Salaries)\n\n# Simple Variable Regression\n\nmodel = lm(Salaries$salary ~ Salaries$yrs.since.phd)\nmodel = lm(salary ~ yrs.since.phd, data = Salaries)\n\nmodel## \n## Call:\n## lm(formula = salary ~ yrs.since.phd, data = Salaries)\n## \n## Coefficients:\n##   (Intercept)  yrs.since.phd  \n##       91718.7          985.3\nsummary(model)## \n## Call:\n## lm(formula = salary ~ yrs.since.phd, data = Salaries)\n## \n## Residuals:\n##    Min     1Q Median     3Q    Max \n## -84171 -19432  -2858  16086 102383 \n## \n## Coefficients:\n##               Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)    91718.7     2765.8  33.162   <2e-16 ***\n## yrs.since.phd    985.3      107.4   9.177   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 27530 on 395 degrees of freedom\n## Multiple R-squared:  0.1758, Adjusted R-squared:  0.1737 \n## F-statistic: 84.23 on 1 and 395 DF,  p-value: < 2.2e-16\nstargazer::stargazer(model, type = \"text\")## \n## ===============================================\n##                         Dependent variable:    \n##                     ---------------------------\n##                               salary           \n## -----------------------------------------------\n## yrs.since.phd               985.342***         \n##                              (107.365)         \n##                                                \n## Constant                   91,718.680***       \n##                             (2,765.792)        \n##                                                \n## -----------------------------------------------\n## Observations                    397            \n## R2                             0.176           \n## Adjusted R2                    0.174           \n## Residual Std. Error    27,533.580 (df = 395)   \n## F Statistic           84.226*** (df = 1; 395)  \n## ===============================================\n## Note:               *p<0.1; **p<0.05; ***p<0.01\n# Multiple Variable Regression\n\nmodel1 = lm(salary ~ yrs.since.phd + yrs.service, data = Salaries)\nsummary(model1)## \n## Call:\n## lm(formula = salary ~ yrs.since.phd + yrs.service, data = Salaries)\n## \n## Residuals:\n##    Min     1Q Median     3Q    Max \n## -79735 -19823  -2617  15149 106149 \n## \n## Coefficients:\n##               Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)    89912.2     2843.6  31.620  < 2e-16 ***\n## yrs.since.phd   1562.9      256.8   6.086 2.75e-09 ***\n## yrs.service     -629.1      254.5  -2.472   0.0138 *  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 27360 on 394 degrees of freedom\n## Multiple R-squared:  0.1883, Adjusted R-squared:  0.1842 \n## F-statistic: 45.71 on 2 and 394 DF,  p-value: < 2.2e-16\n### Model:\n### salary = 89912.2 + 1562.9 * yrs.since.phd + (-629.1) * yrs.service\n\n\n# Categorical Variables\n\ncontrasts(Salaries$sex)##        Male\n## Female    0\n## Male      1\n# sex = relevel(sex, ref = \"Male\")\n\nmodel2 = lm(salary ~ yrs.since.phd + yrs.service + sex, data = Salaries)\nsummary(model2)## \n## Call:\n## lm(formula = salary ~ yrs.since.phd + yrs.service + sex, data = Salaries)\n## \n## Residuals:\n##    Min     1Q Median     3Q    Max \n## -79586 -19564  -3018  15071 105898 \n## \n## Coefficients:\n##               Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)    82875.9     4800.6  17.264  < 2e-16 ***\n## yrs.since.phd   1552.8      256.1   6.062 3.15e-09 ***\n## yrs.service     -649.8      254.0  -2.558   0.0109 *  \n## sexMale         8457.1     4656.1   1.816   0.0701 .  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 27280 on 393 degrees of freedom\n## Multiple R-squared:  0.1951, Adjusted R-squared:  0.189 \n## F-statistic: 31.75 on 3 and 393 DF,  p-value: < 2.2e-16\ncar::Anova(model2)## Anova Table (Type II tests)\n## \n## Response: salary\n##                   Sum Sq  Df F value   Pr(>F)    \n## yrs.since.phd 2.7346e+10   1 36.7512 3.15e-09 ***\n## yrs.service   4.8697e+09   1  6.5447  0.01089 *  \n## sex           2.4547e+09   1  3.2990  0.07008 .  \n## Residuals     2.9242e+11 393                     \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nmodel3 = lm(salary ~ ., data = Salaries)\ncar::Anova(model3)## Anova Table (Type II tests)\n## \n## Response: salary\n##                   Sum Sq  Df F value    Pr(>F)    \n## rank          6.9508e+10   2 68.4143 < 2.2e-16 ***\n## discipline    1.9237e+10   1 37.8695 1.878e-09 ***\n## yrs.since.phd 2.5041e+09   1  4.9293   0.02698 *  \n## yrs.service   2.7100e+09   1  5.3348   0.02143 *  \n## sex           7.8068e+08   1  1.5368   0.21584    \n## Residuals     1.9812e+11 390                      \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nsummary(model3)## \n## Call:\n## lm(formula = salary ~ ., data = Salaries)\n## \n## Residuals:\n##    Min     1Q Median     3Q    Max \n## -65248 -13211  -1775  10384  99592 \n## \n## Coefficients:\n##               Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)    65955.2     4588.6  14.374  < 2e-16 ***\n## rankAssocProf  12907.6     4145.3   3.114  0.00198 ** \n## rankProf       45066.0     4237.5  10.635  < 2e-16 ***\n## disciplineB    14417.6     2342.9   6.154 1.88e-09 ***\n## yrs.since.phd    535.1      241.0   2.220  0.02698 *  \n## yrs.service     -489.5      211.9  -2.310  0.02143 *  \n## sexMale         4783.5     3858.7   1.240  0.21584    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 22540 on 390 degrees of freedom\n## Multiple R-squared:  0.4547, Adjusted R-squared:  0.4463 \n## F-statistic:  54.2 on 6 and 390 DF,  p-value: < 2.2e-16\n# Transformations and Interaction Terms\n\nmodel4 = lm(salary ~ yrs.since.phd^2 + yrs.service, data = Salaries)\nsummary(model4)## \n## Call:\n## lm(formula = salary ~ yrs.since.phd^2 + yrs.service, data = Salaries)\n## \n## Residuals:\n##    Min     1Q Median     3Q    Max \n## -79735 -19823  -2617  15149 106149 \n## \n## Coefficients:\n##               Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)    89912.2     2843.6  31.620  < 2e-16 ***\n## yrs.since.phd   1562.9      256.8   6.086 2.75e-09 ***\n## yrs.service     -629.1      254.5  -2.472   0.0138 *  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 27360 on 394 degrees of freedom\n## Multiple R-squared:  0.1883, Adjusted R-squared:  0.1842 \n## F-statistic: 45.71 on 2 and 394 DF,  p-value: < 2.2e-16\nmodel4 = lm(salary ~ yrs.since.phd + I(yrs.since.phd^2) + yrs.service, data = Salaries)\nsummary(model4)## \n## Call:\n## lm(formula = salary ~ yrs.since.phd + I(yrs.since.phd^2) + yrs.service, \n##     data = Salaries)\n## \n## Residuals:\n##    Min     1Q Median     3Q    Max \n## -63538 -18063  -1946  14919 105025 \n## \n## Coefficients:\n##                     Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)        64971.002   3950.746  16.445  < 2e-16 ***\n## yrs.since.phd       4222.493    394.237  10.711  < 2e-16 ***\n## I(yrs.since.phd^2)   -62.321      7.389  -8.434 6.42e-16 ***\n## yrs.service         -234.596    239.075  -0.981    0.327    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 25210 on 393 degrees of freedom\n## Multiple R-squared:  0.3127, Adjusted R-squared:  0.3075 \n## F-statistic: 59.61 on 3 and 393 DF,  p-value: < 2.2e-16\nmodel4 = lm(salary ~ yrs.since.phd + I(yrs.since.phd^2) + I(yrs.since.phd^3) + yrs.service, data = Salaries)\nsummary(model4)## \n## Call:\n## lm(formula = salary ~ yrs.since.phd + I(yrs.since.phd^2) + I(yrs.since.phd^3) + \n##     yrs.service, data = Salaries)\n## \n## Residuals:\n##    Min     1Q Median     3Q    Max \n## -63538 -18062  -1947  14917 105023 \n## \n## Coefficients:\n##                      Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)         6.498e+04  5.559e+03  11.688  < 2e-16 ***\n## yrs.since.phd       4.221e+03  8.990e+02   4.696 3.68e-06 ***\n## I(yrs.since.phd^2) -6.227e+01  3.877e+01  -1.606    0.109    \n## I(yrs.since.phd^3) -6.720e-04  4.935e-01  -0.001    0.999    \n## yrs.service        -2.346e+02  2.395e+02  -0.979    0.328    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 25240 on 392 degrees of freedom\n## Multiple R-squared:  0.3127, Adjusted R-squared:  0.3057 \n## F-statistic:  44.6 on 4 and 392 DF,  p-value: < 2.2e-16\nmodel4 = lm(I(log(salary)) ~ yrs.since.phd + I(yrs.since.phd^2) + I(yrs.since.phd^3) + yrs.service, data = Salaries)\nsummary(model4)## \n## Call:\n## lm(formula = I(log(salary)) ~ yrs.since.phd + I(yrs.since.phd^2) + \n##     I(yrs.since.phd^3) + yrs.service, data = Salaries)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -0.68247 -0.15590 -0.00244  0.14242  0.74830 \n## \n## Coefficients:\n##                      Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)         1.115e+01  4.617e-02 241.467  < 2e-16 ***\n## yrs.since.phd       4.073e-02  7.467e-03   5.454 8.72e-08 ***\n## I(yrs.since.phd^2) -6.626e-04  3.220e-04  -2.058   0.0403 *  \n## I(yrs.since.phd^3)  6.168e-07  4.099e-06   0.150   0.8805    \n## yrs.service        -1.433e-03  1.989e-03  -0.720   0.4718    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.2096 on 392 degrees of freedom\n## Multiple R-squared:  0.3575, Adjusted R-squared:  0.3509 \n## F-statistic: 54.52 on 4 and 392 DF,  p-value: < 2.2e-16\nmodel5 = lm(salary ~ yrs.since.phd:yrs.service, data = Salaries)\nsummary(model5)## \n## Call:\n## lm(formula = salary ~ yrs.since.phd:yrs.service, data = Salaries)\n## \n## Residuals:\n##    Min     1Q Median     3Q    Max \n## -80936 -21633  -3841  17621 106895 \n## \n## Coefficients:\n##                            Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)               1.071e+05  1.983e+03  53.982  < 2e-16 ***\n## yrs.since.phd:yrs.service 1.218e+01  2.431e+00   5.009 8.26e-07 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 29410 on 395 degrees of freedom\n## Multiple R-squared:  0.05973,    Adjusted R-squared:  0.05735 \n## F-statistic: 25.09 on 1 and 395 DF,  p-value: 8.263e-07\n#### Boxcox\n\nsal = Salaries[, c(3,4,6)]\nshapiro.test(Salaries$salary)## \n##  Shapiro-Wilk normality test\n## \n## data:  Salaries$salary\n## W = 0.95988, p-value = 6.076e-09\n# Null: Data is normally distributed\n# p-value = 6.076e-09 < 0.05, reject null -> NOT Normal.\n\nmodel1 = lm(salary ~ yrs.since.phd + yrs.service, data = Salaries)\nsummary(model1)## \n## Call:\n## lm(formula = salary ~ yrs.since.phd + yrs.service, data = Salaries)\n## \n## Residuals:\n##    Min     1Q Median     3Q    Max \n## -79735 -19823  -2617  15149 106149 \n## \n## Coefficients:\n##               Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)    89912.2     2843.6  31.620  < 2e-16 ***\n## yrs.since.phd   1562.9      256.8   6.086 2.75e-09 ***\n## yrs.service     -629.1      254.5  -2.472   0.0138 *  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 27360 on 394 degrees of freedom\n## Multiple R-squared:  0.1883, Adjusted R-squared:  0.1842 \n## F-statistic: 45.71 on 2 and 394 DF,  p-value: < 2.2e-16\nbc = boxcox(model1)\nbest.lam = bc$x[which(bc$y == max(bc$y))]\nbest.lam## [1] -0.2222222\nmodel6 = lm(I(salary^best.lam) ~ yrs.since.phd + yrs.service, data = Salaries)\nsummary(model6)## \n## Call:\n## lm(formula = I(salary^best.lam) ~ yrs.since.phd + yrs.service, \n##     data = Salaries)\n## \n## Residuals:\n##        Min         1Q     Median         3Q        Max \n## -0.0099656 -0.0027195 -0.0000644  0.0028614  0.0150252 \n## \n## Coefficients:\n##                 Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)    7.942e-02  4.095e-04 193.953  < 2e-16 ***\n## yrs.since.phd -2.260e-04  3.698e-05  -6.110 2.39e-09 ***\n## yrs.service    8.892e-05  3.665e-05   2.426   0.0157 *  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.00394 on 394 degrees of freedom\n## Multiple R-squared:  0.1929, Adjusted R-squared:  0.1888 \n## F-statistic: 47.09 on 2 and 394 DF,  p-value: < 2.2e-16\n### Adj. R^2 increased\n\n# Predictions using Training and Testing data\n\nset.seed(123)\nsplit = sample.split(Salaries$salary, SplitRatio = 0.8)\ntraining_set = subset(Salaries, split == T)\ntest_set = subset(Salaries, split == F)\n\nmodel7 = lm(salary ~ ., data = training_set)\ny_pred = predict(model7, test_set)\n# y_pred\ndata.frame(y_pred, test_set$salary)##        y_pred test_set.salary\n## 2   133991.08          173200\n## 4   138905.04          115000\n## 5   134650.37          141500\n## 8   135930.61          147765\n## 11  100457.55          119800\n## 16  135214.59          117150\n## 20  116504.49          137000\n## 21  121107.75           89565\n## 24  120009.45          113068\n## 31  139939.95          132261\n## 32   89375.54           79916\n## 34   87417.62           80225\n## 50   85955.45           70768\n## 53   86623.38           74692\n## 59   98656.53          100135\n## 65   86921.88           68404\n## 67  137279.32          101000\n## 68  136344.57           99418\n## 69  126670.46          111512\n## 84   88722.90           88825\n## 87  134675.41          152708\n## 88   86112.35           88400\n## 89  132792.62          172272\n## 104 130115.59          127512\n## 106 120116.27          113543\n## 107  84699.94           82099\n## 111 118886.12          112429\n## 114 119570.45          104279\n## 118 121371.46          117515\n## 126 124716.43           78162\n## 132 122055.79           76840\n## 137 117267.04          108262\n## 139  84543.04           73877\n## 145 133106.42          112696\n## 151 132058.21          128148\n## 173 141120.02           93164\n## 179 139551.03          147349\n## 181 130596.04          142467\n## 189 100984.98          106300\n## 190 135767.06          153750\n## 193 132346.97          122100\n## 195 101644.27           90000\n## 202 135146.11          119700\n## 206 141584.07           96545\n## 219  97391.59          109650\n## 220 131901.31          119500\n## 222 138923.43          145200\n## 230 120379.98          133900\n## 238  67420.64           63100\n## 240 123190.87           96200\n## 248 118547.28          101100\n## 249 120637.05          128800\n## 260 119777.43           92550\n## 261  91885.61           88600\n## 262 120825.64          107550\n## 264 118629.05          126000\n## 271 121346.42          143250\n## 277 123906.89          107200\n## 294  88170.11          104800\n## 296 122024.10           97150\n## 297 116589.36          126300\n## 300  91521.73           70700\n## 316  88227.16           84716\n## 317  95094.83           71065\n## 320 131876.27          135027\n## 321 133131.46          104428\n## 327 136444.74          124714\n## 330 132478.83          134778\n## 334 140988.16          145098\n## 340 145581.67          137317\n## 347 142243.36          142023\n## 352 134832.31           93519\n## 356 134775.58          145028\n## 360  75889.64           78785\n## 363 118472.15          138771\n## 373 118126.66          109707\n## 376 119149.83          103649\n## 380  84699.94          104121\n## 386 119093.10          114330\n## 391 130451.66          166605\n# Variable Selection\n\n# data\ndata(\"swiss\")\nattach(swiss)\n\n# ?swiss\n\n# Best Subsets regression\n\nmodels = leaps::regsubsets(Fertility ~ ., data = swiss, nvmax = 5)\nsummary(models)## Subset selection object\n## Call: regsubsets.formula(Fertility ~ ., data = swiss, nvmax = 5)\n## 5 Variables  (and intercept)\n##                  Forced in Forced out\n## Agriculture          FALSE      FALSE\n## Examination          FALSE      FALSE\n## Education            FALSE      FALSE\n## Catholic             FALSE      FALSE\n## Infant.Mortality     FALSE      FALSE\n## 1 subsets of each size up to 5\n## Selection Algorithm: exhaustive\n##          Agriculture Examination Education Catholic Infant.Mortality\n## 1  ( 1 ) \" \"         \" \"         \"*\"       \" \"      \" \"             \n## 2  ( 1 ) \" \"         \" \"         \"*\"       \"*\"      \" \"             \n## 3  ( 1 ) \" \"         \" \"         \"*\"       \"*\"      \"*\"             \n## 4  ( 1 ) \"*\"         \" \"         \"*\"       \"*\"      \"*\"             \n## 5  ( 1 ) \"*\"         \"*\"         \"*\"       \"*\"      \"*\"\n### Therefore, \n### Best 1-variable model: Fertility ~ Education\n### Best 2-variables model: Fertility ~ Education + Catholic\n### Best 3-variables model: Fertility ~ Education + Catholic + Infant.Mortality\n### Best 4-variables model: Fertility ~ Agriculture + Education + Catholic + Infant.Mortality\n### Best 5-variables model: Fertility ~ Agriculture + Examination + Education + Catholic + Infant.Mortality\n\nmodels.summary = summary(models)\ndata.frame(Adj.R2 = which.max(models.summary$adjr2),\n           CP = which.min(models.summary$cp),\n           BIC = which.min(models.summary$bic))##   Adj.R2 CP BIC\n## 1      5  4   4\n### Fertility ~ Agriculture + Education + Catholic + Infant.Mortality\n\n# Stepwise Variable Selection\nfit = lm(Fertility ~ ., data = swiss)\nstep = MASS::stepAIC(fit, direction = \"both\", trace = F) # change both to forward and backward\nstep## \n## Call:\n## lm(formula = Fertility ~ Agriculture + Education + Catholic + \n##     Infant.Mortality, data = swiss)\n## \n## Coefficients:\n##      (Intercept)       Agriculture         Education          Catholic  \n##          62.1013           -0.1546           -0.9803            0.1247  \n## Infant.Mortality  \n##           1.0784\ndetach(swiss)\n\n\n# Penalized Regression\n\names = read.csv(\"https://raw.githubusercontent.com/Parv-Joshi/EDAV_CC_Datasets/main/Ames_Housing_Data.csv\")\n# str(ames)\nanyNA(ames)## [1] FALSE\nset.seed(123)\ntraining.samples = createDataPartition(ames$SalePrice, p = 0.75, list = FALSE)\n\ntrain.data = ames[training.samples,]\ntest.data = ames[-training.samples,]\n\nlambda = 10^seq(-3, 3, length = 100)\n\n# Ridge Regression\nset.seed(123)\nridge = train(SalePrice ~ ., data = train.data, method = \"glmnet\",\n  trControl = trainControl(\"cv\", number = 10),\n  tuneGrid = expand.grid(alpha = 0, lambda = lambda))\n\n# LASSO\nset.seed(123)\nlasso = train(SalePrice ~ ., data = train.data, method = \"glmnet\",\n  trControl = trainControl(\"cv\", number = 10),\n  tuneGrid = expand.grid(alpha = 1, lambda = lambda))\n\n# Elastic Net\nset.seed(123)\nelastic = train(SalePrice ~ ., data = train.data, method = \"glmnet\",\n  trControl = trainControl(\"cv\", number = 10),\n  tuneLength = 10)\n\n# Comparison\nmodels = list(ridge = ridge, lasso = lasso, elastic = elastic)\nresamples(models) %>% summary(metric = \"RMSE\")## \n## Call:\n## summary.resamples(object = ., metric = \"RMSE\")\n## \n## Models: ridge, lasso, elastic \n## Number of resamples: 10 \n## \n## RMSE \n##             Min.  1st Qu.   Median     Mean  3rd Qu.     Max. NA's\n## ridge   24044.91 29603.57 30799.80 32919.29 37210.80 45150.33    0\n## lasso   23614.27 29604.97 31077.67 32800.93 37787.06 44271.58    0\n## elastic 23725.92 29688.82 31124.32 32770.19 37752.31 44115.51    0\n# Since Elastic model has the lowest mean RMSE, we can conclude that the Elastic model is the best."},{"path":"regression-and-classification-in-r.html","id":"classification","chapter":"14 Regression and Classification in R","heading":"14.0.5 Classification","text":"","code":"\n# Data\n\ndata(\"PimaIndiansDiabetes2\", package = \"mlbench\")\n\n# str(PimaIndiansDiabetes2)\n# View(PimaIndiansDiabetes2)\n\nPimaIndiansDiabetes2$diabetes = as.factor(PimaIndiansDiabetes2$diabetes)\nPimaIndiansDiabetes2 = na.omit(PimaIndiansDiabetes2)\n\nattach(PimaIndiansDiabetes2)\n\n# Training and Testing\n\nset.seed(123)\n\ntraining.samples = createDataPartition(diabetes, p = 0.8, list = FALSE)\n\ntrain.data = PimaIndiansDiabetes2[training.samples,]\ntest.data = PimaIndiansDiabetes2[-training.samples,]\n\n# Logistic Regression\n\nmodel = glm(diabetes ~ ., data = train.data, family = binomial)\nsummary(model)## \n## Call:\n## glm(formula = diabetes ~ ., family = binomial, data = train.data)\n## \n## Deviance Residuals: \n##     Min       1Q   Median       3Q      Max  \n## -2.5832  -0.6544  -0.3292   0.6248   2.5968  \n## \n## Coefficients:\n##               Estimate Std. Error z value Pr(>|z|)    \n## (Intercept) -1.053e+01  1.440e+00  -7.317 2.54e-13 ***\n## pregnant     1.005e-01  6.127e-02   1.640  0.10092    \n## glucose      3.710e-02  6.486e-03   5.719 1.07e-08 ***\n## pressure    -3.876e-04  1.383e-02  -0.028  0.97764    \n## triceps      1.418e-02  1.998e-02   0.710  0.47800    \n## insulin      5.940e-04  1.508e-03   0.394  0.69371    \n## mass         7.997e-02  3.180e-02   2.515  0.01190 *  \n## pedigree     1.329e+00  4.823e-01   2.756  0.00585 ** \n## age          2.718e-02  2.020e-02   1.346  0.17840    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 398.80  on 313  degrees of freedom\n## Residual deviance: 267.18  on 305  degrees of freedom\n## AIC: 285.18\n## \n## Number of Fisher Scoring iterations: 5\nprobabilities = predict(model, test.data, type = \"response\")\nprobabilities##          19          21          32          55          64          71 \n## 0.192628377 0.485262263 0.662527248 0.798681474 0.278073391 0.145877334 \n##          72          74          98          99         108         111 \n## 0.314265178 0.232071188 0.007697533 0.066394837 0.357546947 0.552586956 \n##         115         128         154         182         215         216 \n## 0.715548197 0.132717063 0.706670106 0.189297618 0.291196167 0.911862874 \n##         224         229         260         293         297         313 \n## 0.704569592 0.993419157 0.932506403 0.721153294 0.328274489 0.293808296 \n##         316         326         357         369         376         385 \n## 0.120862955 0.201559764 0.390156419 0.022075579 0.885924184 0.075765425 \n##         386         393         394         410         429         447 \n## 0.042383376 0.102435789 0.095664525 0.885380718 0.379195274 0.053098974 \n##         450         453         468         470         477         487 \n## 0.091641699 0.097155564 0.122327231 0.831420989 0.216021458 0.525840641 \n##         541         542         546         552         555         556 \n## 0.461122260 0.270914264 0.890122464 0.066719675 0.068520682 0.197336318 \n##         562         563         564         577         589         592 \n## 0.894087110 0.075000107 0.091244654 0.163897301 0.912857186 0.200938223 \n##         595         600         609         610         621         634 \n## 0.491041316 0.048192839 0.549602575 0.034910473 0.203922043 0.081878938 \n##         666         673         674         681         683         694 \n## 0.133609108 0.100033198 0.782544310 0.007547670 0.145787456 0.629221735 \n##         697         699         710         716         717         719 \n## 0.485455842 0.290737653 0.141965217 0.925604098 0.839268863 0.161190610 \n##         722         731         733         734         746         766 \n## 0.168129887 0.191170873 0.852375783 0.078840151 0.305248512 0.125461309\ncontrasts(diabetes)##     pos\n## neg   0\n## pos   1\npredicted.classes = ifelse(probabilities > 0.5, \"pos\", \"neg\")\npredicted.classes##    19    21    32    55    64    71    72    74    98    99   108   111   115 \n## \"neg\" \"neg\" \"pos\" \"pos\" \"neg\" \"neg\" \"neg\" \"neg\" \"neg\" \"neg\" \"neg\" \"pos\" \"pos\" \n##   128   154   182   215   216   224   229   260   293   297   313   316   326 \n## \"neg\" \"pos\" \"neg\" \"neg\" \"pos\" \"pos\" \"pos\" \"pos\" \"pos\" \"neg\" \"neg\" \"neg\" \"neg\" \n##   357   369   376   385   386   393   394   410   429   447   450   453   468 \n## \"neg\" \"neg\" \"pos\" \"neg\" \"neg\" \"neg\" \"neg\" \"pos\" \"neg\" \"neg\" \"neg\" \"neg\" \"neg\" \n##   470   477   487   541   542   546   552   555   556   562   563   564   577 \n## \"pos\" \"neg\" \"pos\" \"neg\" \"neg\" \"pos\" \"neg\" \"neg\" \"neg\" \"pos\" \"neg\" \"neg\" \"neg\" \n##   589   592   595   600   609   610   621   634   666   673   674   681   683 \n## \"pos\" \"neg\" \"neg\" \"neg\" \"pos\" \"neg\" \"neg\" \"neg\" \"neg\" \"neg\" \"pos\" \"neg\" \"neg\" \n##   694   697   699   710   716   717   719   722   731   733   734   746   766 \n## \"pos\" \"neg\" \"neg\" \"neg\" \"pos\" \"pos\" \"neg\" \"neg\" \"neg\" \"pos\" \"neg\" \"neg\" \"neg\"\ncaret::confusionMatrix(factor(predicted.classes),\n                factor(test.data$diabetes),\n                positive = \"pos\")## Confusion Matrix and Statistics\n## \n##           Reference\n## Prediction neg pos\n##        neg  44  11\n##        pos   8  15\n##                                          \n##                Accuracy : 0.7564         \n##                  95% CI : (0.646, 0.8465)\n##     No Information Rate : 0.6667         \n##     P-Value [Acc > NIR] : 0.05651        \n##                                          \n##                   Kappa : 0.4356         \n##                                          \n##  Mcnemar's Test P-Value : 0.64636        \n##                                          \n##             Sensitivity : 0.5769         \n##             Specificity : 0.8462         \n##          Pos Pred Value : 0.6522         \n##          Neg Pred Value : 0.8000         \n##              Prevalence : 0.3333         \n##          Detection Rate : 0.1923         \n##    Detection Prevalence : 0.2949         \n##       Balanced Accuracy : 0.7115         \n##                                          \n##        'Positive' Class : pos            \n## \n# Stepwise regression\n\nstep = MASS::stepAIC(model, direction = \"both\", k = log(nrow(PimaIndiansDiabetes2)), trace = FALSE)\nstep$anova## Stepwise Model Path \n## Analysis of Deviance Table\n## \n## Initial Model:\n## diabetes ~ pregnant + glucose + pressure + triceps + insulin + \n##     mass + pedigree + age\n## \n## Final Model:\n## diabetes ~ pregnant + glucose + mass + pedigree\n## \n## \n##         Step Df     Deviance Resid. Df Resid. Dev      AIC\n## 1                                  305   267.1825 320.9239\n## 2 - pressure  1 0.0007857024       306   267.1833 314.9534\n## 3  - insulin  1 0.1591672501       307   267.3425 309.1413\n## 4  - triceps  1 0.4434205054       308   267.7859 303.6135\n## 5      - age  1 2.4276790188       309   270.2136 300.0699\n# Best subset regression\n\ncv_data = model.matrix( ~ ., PimaIndiansDiabetes2)[,-1]\ncv_data = data.frame(cv_data)\nbest = bestglm(cv_data, IC = \"BIC\", family = binomial)\nbest## BIC\n## BICq equivalent for q in (0.359009418385306, 0.859446547266463)\n## Best Model:\n##                 Estimate  Std. Error   z value     Pr(>|z|)\n## (Intercept) -10.09201799 1.080251137 -9.342289 9.427384e-21\n## glucose       0.03618899 0.004981946  7.264026 3.757357e-13\n## mass          0.07444854 0.020266697  3.673442 2.393046e-04\n## pedigree      1.08712862 0.419408437  2.592052 9.540525e-03\n## age           0.05301206 0.013439480  3.944502 7.996582e-05\ndetach(PimaIndiansDiabetes2)\n\n# Decision Tree Classification\n\ndata = read.csv(\"https://raw.githubusercontent.com/Parv-Joshi/EDAV_CC_Datasets/main/Titanic.csv\")\nattach(data)\n\n# str(data)\n\n# Excluding Variables\ndata = subset(data, select = -c(Name, Ticket, Cabin))\n\n# Removing Missing Data\ndata = subset(data, !is.na(Age))\n\n# Testing and Training set\n\nset.seed(123)\ntraining.samples = data$Survived %>% \n  createDataPartition(p = 0.8, list = FALSE)\n\ntrain.data = data[training.samples,]\ntest.data = data[-training.samples,]\n\n# Factoring Survived\ntrain.data$Survived = as.factor(train.data$Survived)\ntest.data$Survived = as.factor(test.data$Survived)\n\n# Decision Trees\nmodel = rpart::rpart(Survived ~ ., data = train.data, control = rpart.control(cp = 0))\nrattle::fancyRpartPlot(model, cex = 0.5)\nset.seed(123)\ntrain.data$Survived = as.factor(train.data$Survived)\nmodel2 = train(Survived ~ ., \n               data = train.data, \n               method = \"rpart\", \n               trControl = trainControl(\"cv\", number = 10), \n               tuneLength = 100)\n\nfancyRpartPlot(model2$finalModel, cex = 0.6)\nprobabilities = predict(model2, newdata = test.data)\n# we don't need to do  contrasts since Survived is already given in o and 1.\npredicted.classes = ifelse(probabilities == 1, \"1\", \"0\")\n\ncaret::confusionMatrix(factor(predicted.classes),\n                factor(test.data$Survived),\n                positive = \"1\")## Confusion Matrix and Statistics\n## \n##           Reference\n## Prediction  0  1\n##          0 74 19\n##          1  5 44\n##                                          \n##                Accuracy : 0.831          \n##                  95% CI : (0.759, 0.8886)\n##     No Information Rate : 0.5563         \n##     P-Value [Acc > NIR] : 3.722e-12      \n##                                          \n##                   Kappa : 0.6497         \n##                                          \n##  Mcnemar's Test P-Value : 0.007963       \n##                                          \n##             Sensitivity : 0.6984         \n##             Specificity : 0.9367         \n##          Pos Pred Value : 0.8980         \n##          Neg Pred Value : 0.7957         \n##              Prevalence : 0.4437         \n##          Detection Rate : 0.3099         \n##    Detection Prevalence : 0.3451         \n##       Balanced Accuracy : 0.8176         \n##                                          \n##        'Positive' Class : 1              \n## \n# Random Forest\n\nset.seed(123)\nmodel3 = train(Survived ~ ., \n              data = train.data, \n              method = \"rf\",\n              trControl = trainControl(\"cv\", number = 10),\n              importance = TRUE)\n\nprobabilities = predict(model3, newdata = test.data)\npredicted.classes = ifelse(probabilities == 1, \"1\", \"0\")\ncaret::confusionMatrix(factor(predicted.classes),\n                factor(test.data$Survived),\n                positive = \"1\")## Confusion Matrix and Statistics\n## \n##           Reference\n## Prediction  0  1\n##          0 76 21\n##          1  3 42\n##                                          \n##                Accuracy : 0.831          \n##                  95% CI : (0.759, 0.8886)\n##     No Information Rate : 0.5563         \n##     P-Value [Acc > NIR] : 3.722e-12      \n##                                          \n##                   Kappa : 0.6474         \n##                                          \n##  Mcnemar's Test P-Value : 0.0005202      \n##                                          \n##             Sensitivity : 0.6667         \n##             Specificity : 0.9620         \n##          Pos Pred Value : 0.9333         \n##          Neg Pred Value : 0.7835         \n##              Prevalence : 0.4437         \n##          Detection Rate : 0.2958         \n##    Detection Prevalence : 0.3169         \n##       Balanced Accuracy : 0.8143         \n##                                          \n##        'Positive' Class : 1              \n## \nrandomForest::varImpPlot(model3$finalModel, type = 1) # MeanDecreaseAccuracy\ncaret::varImp(model3, type = 1)## rf variable importance\n## \n##             Overall\n## Sexmale     100.000\n## Pclass       54.019\n## Fare         38.444\n## Age          37.316\n## SibSp        24.658\n## Parch        19.187\n## EmbarkedQ     4.655\n## EmbarkedS     3.630\n## EmbarkedC     3.560\n## PassengerId   0.000\nrandomForest::varImpPlot(model3$finalModel, type = 2) # MeanDecreaseGini\ncaret::varImp(model3, type = 2)## rf variable importance\n## \n##             Overall\n## Sexmale     100.000\n## Fare         62.753\n## Age          54.613\n## PassengerId  48.686\n## Pclass       35.447\n## SibSp        15.503\n## Parch        14.097\n## EmbarkedS     3.623\n## EmbarkedC     3.606\n## EmbarkedQ     0.000\ndetach(data)"},{"path":"tibble-vs.-dataframe.html","id":"tibble-vs.-dataframe","chapter":"15 Tibble vs. DataFrame","heading":"15 Tibble vs. DataFrame","text":"Jingfei Fang","code":"\nlibrary(tidyverse)\nlibrary(tibble)"},{"path":"tibble-vs.-dataframe.html","id":"introduction","chapter":"15 Tibble vs. DataFrame","heading":"15.0.1 Introduction","text":"tibble often considered neater format data frame, often used tidyverse ggplot2 packages. contains information data frame, manipulation representation tibbles different data frames aspects.","code":""},{"path":"tibble-vs.-dataframe.html","id":"getting-started-with-tibbles","chapter":"15 Tibble vs. DataFrame","heading":"15.0.2 1. Getting started with tibbles","text":"can tidyverse:can installing tibble package directly:","code":"\n#install.packages(\"tidyverse\")\nlibrary(tidyverse)\n#install.packages(\"tibble\")\nlibrary(tibble)"},{"path":"tibble-vs.-dataframe.html","id":"creating-a-tibble","chapter":"15 Tibble vs. DataFrame","heading":"15.0.3 2. Creating a tibble","text":"can create tibble directly:can create tibble existing data frame using as_tibble(). use ‘iris’ dataset example:","code":"\ntib <- tibble(a = c(1,2,3), b = c(4,5,6), c = c(7,8,9))\ntib## # A tibble: 3 × 3\n##       a     b     c\n##   <dbl> <dbl> <dbl>\n## 1     1     4     7\n## 2     2     5     8\n## 3     3     6     9\ndf <- iris\nclass(df)## [1] \"data.frame\"\ntib <- as_tibble(df)\ntib## # A tibble: 150 × 5\n##    Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n##           <dbl>       <dbl>        <dbl>       <dbl> <fct>  \n##  1          5.1         3.5          1.4         0.2 setosa \n##  2          4.9         3            1.4         0.2 setosa \n##  3          4.7         3.2          1.3         0.2 setosa \n##  4          4.6         3.1          1.5         0.2 setosa \n##  5          5           3.6          1.4         0.2 setosa \n##  6          5.4         3.9          1.7         0.4 setosa \n##  7          4.6         3.4          1.4         0.3 setosa \n##  8          5           3.4          1.5         0.2 setosa \n##  9          4.4         2.9          1.4         0.2 setosa \n## 10          4.9         3.1          1.5         0.1 setosa \n## # … with 140 more rows"},{"path":"tibble-vs.-dataframe.html","id":"unlike-data-frames-tibbles-dont-show-the-entire-dataset-when-you-print-it.","chapter":"15 Tibble vs. DataFrame","heading":"15.0.4 3. Unlike data frames, tibbles don’t show the entire dataset when you print it.","text":"","code":"\ntib## # A tibble: 150 × 5\n##    Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n##           <dbl>       <dbl>        <dbl>       <dbl> <fct>  \n##  1          5.1         3.5          1.4         0.2 setosa \n##  2          4.9         3            1.4         0.2 setosa \n##  3          4.7         3.2          1.3         0.2 setosa \n##  4          4.6         3.1          1.5         0.2 setosa \n##  5          5           3.6          1.4         0.2 setosa \n##  6          5.4         3.9          1.7         0.4 setosa \n##  7          4.6         3.4          1.4         0.3 setosa \n##  8          5           3.4          1.5         0.2 setosa \n##  9          4.4         2.9          1.4         0.2 setosa \n## 10          4.9         3.1          1.5         0.1 setosa \n## # … with 140 more rows"},{"path":"tibble-vs.-dataframe.html","id":"tibbles-cannot-access-a-column-when-you-provide-a-partial-name-of-the-column-but-data-frames-can.","chapter":"15 Tibble vs. DataFrame","heading":"15.0.5 4. Tibbles cannot access a column when you provide a partial name of the column, but data frames can.","text":"","code":""},{"path":"tibble-vs.-dataframe.html","id":"tibble","chapter":"15 Tibble vs. DataFrame","heading":"15.0.5.1 Tibble","text":"try match column name partial name, work.provide entire column name, work.","code":"\ntib <- tibble(str = c(\"a\",\"b\",\"c\",\"d\"), int = c(1,2,3,4))\ntib$st## NULL\ntib$str## [1] \"a\" \"b\" \"c\" \"d\""},{"path":"tibble-vs.-dataframe.html","id":"data-frame","chapter":"15 Tibble vs. DataFrame","heading":"15.0.5.2 Data Frame","text":"However, can access “str” column providing partial column name “st” (long partial name unique).","code":"\ndf <- data.frame(str = c(\"a\",\"b\",\"c\",\"d\"), int = c(1,2,3,4))\ndf$st## [1] \"a\" \"b\" \"c\" \"d\""},{"path":"tibble-vs.-dataframe.html","id":"when-you-access-only-one-column-of-a-tibble-it-will-keep-the-tibble-structure.-but-when-you-access-one-column-of-a-data-frame-it-will-become-a-vector.","chapter":"15 Tibble vs. DataFrame","heading":"15.0.6 5. When you access only one column of a tibble, it will keep the tibble structure. But when you access one column of a data frame, it will become a vector.","text":"","code":""},{"path":"tibble-vs.-dataframe.html","id":"tibble-1","chapter":"15 Tibble vs. DataFrame","heading":"15.0.6.1 Tibble","text":"Checking ’s still tibble:can see tibble structure preserved.","code":"\ntib[,\"str\"]## # A tibble: 4 × 1\n##   str  \n##   <chr>\n## 1 a    \n## 2 b    \n## 3 c    \n## 4 d\nis_tibble(tib[,\"str\"])## [1] TRUE"},{"path":"tibble-vs.-dataframe.html","id":"data-frame-1","chapter":"15 Tibble vs. DataFrame","heading":"15.0.6.2 Data Frame","text":"Checking ’s still data frame:’s longer data frame.","code":"\ndf[,\"str\"]## [1] \"a\" \"b\" \"c\" \"d\"\nis.data.frame(df[,\"str\"])## [1] FALSE"},{"path":"tibble-vs.-dataframe.html","id":"however-other-forms-of-subsetting-including-and-work-the-same-for-tibbles-and-data-frames.","chapter":"15 Tibble vs. DataFrame","heading":"15.0.6.3 However, other forms of subsetting, including [[ ]] and $, work the same for tibbles and data frames.","text":"can see subsetting [[ ]] $ also don’t preserve tibble structure.","code":"\ntib[[\"str\"]]## [1] \"a\" \"b\" \"c\" \"d\"\ndf[[\"str\"]]## [1] \"a\" \"b\" \"c\" \"d\"\ntib$str## [1] \"a\" \"b\" \"c\" \"d\"\ndf$str## [1] \"a\" \"b\" \"c\" \"d\""},{"path":"tibble-vs.-dataframe.html","id":"when-assigning-a-new-column-to-a-tibble-the-input-will-not-be-recycled-which-means-you-have-to-provide-an-input-of-the-same-length-of-the-other-columns.-but-a-data-frame-will-recycle-the-input.","chapter":"15 Tibble vs. DataFrame","heading":"15.0.7 6. When assigning a new column to a tibble, the input will not be recycled, which means you have to provide an input of the same length of the other columns. But a data frame will recycle the input.","text":"","code":""},{"path":"tibble-vs.-dataframe.html","id":"tibble-2","chapter":"15 Tibble vs. DataFrame","heading":"15.0.7.1 Tibble","text":"gives error tibble columns length 4, input (5,6) length 2 recycled.\nprovide input length:","code":"\ntib## # A tibble: 4 × 2\n##   str     int\n##   <chr> <dbl>\n## 1 a         1\n## 2 b         2\n## 3 c         3\n## 4 d         4\ntib$newcol <- c(5,6)## Error: Assigned data `c(5, 6)` must be compatible with existing data.\n## ✖ Existing data has 4 rows.\n## ✖ Assigned data has 2 rows.\n## ℹ Only vectors of size 1 are recycled.\ntib$newcol <- rep(c(5,6),2)\ntib## # A tibble: 4 × 3\n##   str     int newcol\n##   <chr> <dbl>  <dbl>\n## 1 a         1      5\n## 2 b         2      6\n## 3 c         3      5\n## 4 d         4      6"},{"path":"tibble-vs.-dataframe.html","id":"data-frame-2","chapter":"15 Tibble vs. DataFrame","heading":"15.0.7.2 Data Frame","text":"Data frames recycle input.","code":"\ndf##   str int\n## 1   a   1\n## 2   b   2\n## 3   c   3\n## 4   d   4\ndf$newcol <- c(5,6)\ndf##   str int newcol\n## 1   a   1      5\n## 2   b   2      6\n## 3   c   3      5\n## 4   d   4      6"},{"path":"tibble-vs.-dataframe.html","id":"reading-with-builtin-read.csv-function-will-output-data-frames-while-reading-with-read_csv-in-readr-package-inside-tidyverse-will-output-tibbles.","chapter":"15 Tibble vs. DataFrame","heading":"15.0.8 7. Reading with builtin read.csv() function will output data frames, while reading with read_csv() in “readr” package inside tidyverse will output tibbles.","text":"","code":""},{"path":"tibble-vs.-dataframe.html","id":"reading-csv-file-with-read.csv","chapter":"15 Tibble vs. DataFrame","heading":"15.0.8.1 Reading csv file with read.csv()","text":"","code":"\ndata <- read.csv(\"https://people.sc.fsu.edu/~jburkardt/data/csv/addresses.csv\")\nclass(data)## [1] \"data.frame\""},{"path":"tibble-vs.-dataframe.html","id":"reading-csv-file-with-read_csv","chapter":"15 Tibble vs. DataFrame","heading":"15.0.8.2 Reading csv file with read_csv()","text":"","code":"\ndata <- read_csv(\"https://people.sc.fsu.edu/~jburkardt/data/csv/addresses.csv\")\nclass(data)## [1] \"spec_tbl_df\" \"tbl_df\"      \"tbl\"         \"data.frame\""},{"path":"tibble-vs.-dataframe.html","id":"tibbles-dont-support-support-arithmetic-operations-on-all-columns-well-the-result-will-be-converted-into-a-data-frame-without-any-notice.","chapter":"15 Tibble vs. DataFrame","heading":"15.0.9 8. Tibbles don’t support support arithmetic operations on all columns well, the result will be converted into a data frame without any notice.","text":"","code":""},{"path":"tibble-vs.-dataframe.html","id":"tibble-3","chapter":"15 Tibble vs. DataFrame","heading":"15.0.9.1 Tibble","text":"can see try multiply elements tibble 2, result correct turned data frame without notifications.","code":"\ntib <- tibble(a = c(1,2,3), b = c(4,5,6), c = c(7,8,9))\nclass(tib*2)## [1] \"data.frame\""},{"path":"tibble-vs.-dataframe.html","id":"data-frame-3","chapter":"15 Tibble vs. DataFrame","heading":"15.0.9.2 Data Frame","text":"data frames issue , converted type.","code":"\ndf <- data.frame(a = c(1,2,3), b = c(4,5,6), c = c(7,8,9))\nclass(df*2)## [1] \"data.frame\""},{"path":"tibble-vs.-dataframe.html","id":"tibbles-preserve-all-the-variable-types-while-data-frames-have-the-option-to-convert-string-into-factor.-in-older-versions-of-r-data-frames-will-convert-string-into-factor-by-default","chapter":"15 Tibble vs. DataFrame","heading":"15.0.10 9. Tibbles preserve all the variable types, while data frames have the option to convert string into factor. (In older versions of R, data frames will convert string into factor by default)","text":"","code":""},{"path":"tibble-vs.-dataframe.html","id":"tibble-4","chapter":"15 Tibble vs. DataFrame","heading":"15.0.10.1 Tibble","text":"can see original data types variables preserved tibble.","code":"\ntib <- tibble(str = c(\"a\",\"b\",\"c\",\"d\"), int = c(1,2,3,4))\nstr(tib)## tibble [4 × 2] (S3: tbl_df/tbl/data.frame)\n##  $ str: chr [1:4] \"a\" \"b\" \"c\" \"d\"\n##  $ int: num [1:4] 1 2 3 4"},{"path":"tibble-vs.-dataframe.html","id":"data-frame-4","chapter":"15 Tibble vs. DataFrame","heading":"15.0.10.2 Data Frame","text":"use data frame, also preserve original types, “stringAsFactors = FALSE” default new versions R.However, also option convert string factor creating data frame setting “stringAsFactors = TRUE”.can see “str” column converted factor.","code":"\ndf <- data.frame(str = c(\"a\",\"b\",\"c\",\"d\"), int = c(1,2,3,4))\nstr(df)## 'data.frame':    4 obs. of  2 variables:\n##  $ str: chr  \"a\" \"b\" \"c\" \"d\"\n##  $ int: num  1 2 3 4\ndf <- data.frame(str = c(\"a\",\"b\",\"c\",\"d\"), int = c(1,2,3,4), stringsAsFactors = TRUE)\nclass(df$str)## [1] \"factor\""},{"path":"tibble-vs.-dataframe.html","id":"tibbles-work-well-with-ggplot2-just-like-data-frames.","chapter":"15 Tibble vs. DataFrame","heading":"15.0.11 10. Tibbles work well with ggplot2, just like data frames.","text":"","code":""},{"path":"tibble-vs.-dataframe.html","id":"tibble-5","chapter":"15 Tibble vs. DataFrame","heading":"15.0.11.1 Tibble:","text":"","code":"\nggplot(data = tib, mapping = aes(x=str, y=int)) +\n  geom_col(width = 0.3)"},{"path":"tibble-vs.-dataframe.html","id":"data-frame-5","chapter":"15 Tibble vs. DataFrame","heading":"15.0.11.2 Data Frame:","text":"","code":"\nggplot(data = df, mapping = aes(x=str, y=int)) +\n  geom_col(width = 0.3)"},{"path":"tibble-vs.-dataframe.html","id":"works-cited","chapter":"15 Tibble vs. DataFrame","heading":"15.1 Works Cited","text":"https://tibble.tidyverse.org/https://cran.r-project.org/web/packages/tibble/vignettes/tibble.htmlhttps://www.youtube.com/watch?v=_qHdqWx-vsQ&ab_channel=JoshuaFrench","code":""},{"path":"introduction-to-time-series.html","id":"introduction-to-time-series","chapter":"16 Introduction to Time Series","heading":"16 Introduction to Time Series","text":"Parth Gupta (pg2677)writing tutorial time series forecasting covering classical techniques time series forecasting.","code":""},{"path":"introduction-to-time-series.html","id":"motivation","chapter":"16 Introduction to Time Series","heading":"16.1 Motivation,","text":"Modeling temporal processes always one important problems. wide variety applications, ranging stock price prediction weather forecasting forecasting web traffic website.three types temporal data,Time SeriesTime SeriesPanelPanelCross SectionalCross SectionalIn time series data one individual observed different time steps. Panel data multiple individuals observed different time steps. Cross sectional data different individuals observed time step.Time series analysis comprises methods analyzing time series data order extract meaningful statistics characteristics data. Time series forecasting use model predict future values based previously observed values. Time series forecasting useful lot real life applications.Visualizing time series data important able get much better understanding underlying patterns data. Like trends increase decrease price item time, seasonal changes like people watch movies weekends. far prediction actual truth.using birth per month dataset. tune parameters different models based training period Jan 1946 Dec 1951. forecast next year. generally use Mean Absolute Percentage Error Root Mean Squared Error evaluate model.$$MAPE = _{= 0}^{N}|( - y_i)|/ y_i\\RMSE = (_{= 0}^{N}( - y_i)^2/ N)^{1/2}$$Every time series three components, trends, seasonality residualsTime Series = Trend + Seasonality + ResidualsTrend, refers overall general direction data, obtained ignoring short term effects seasonal variations noise.Trend, refers overall general direction data, obtained ignoring short term effects seasonal variations noise.Seasonality, refers periodic fluctuations repeated throughout time series period.Seasonality, refers periodic fluctuations repeated throughout time series period.Residuals, refers left part trend seasonality.Residuals, refers left part trend seasonality.plots trying visualize seasonality aspect time series data. plotted data month whole training period.","code":"\ndf <-births <- scan(\"http://robjhyndman.com/tsdldata/data/nybirths.dat\")\ndata <- df[1:72]\nfuture <- df[73:84]\nprint(data)##  [1] 26.663 23.598 26.931 24.740 25.806 24.364 24.477 23.901 23.175 23.227\n## [11] 21.672 21.870 21.439 21.089 23.709 21.669 21.752 20.761 23.479 23.824\n## [21] 23.105 23.110 21.759 22.073 21.937 20.035 23.590 21.672 22.222 22.123\n## [31] 23.950 23.504 22.238 23.142 21.059 21.573 21.548 20.000 22.424 20.615\n## [41] 21.761 22.874 24.104 23.748 23.262 22.907 21.519 22.025 22.604 20.894\n## [51] 24.677 23.673 25.320 23.583 24.671 24.454 24.122 24.252 22.084 22.991\n## [61] 23.287 23.049 25.076 24.037 24.430 24.667 26.451 25.618 25.014 25.110\n## [71] 22.964 23.981\ntime_series <- ts(data, start=c(1946, 1), end=c(1951, 12), frequency=12)\nplot(time_series)\nfit <- stl(time_series, s.window=\"period\")\nplot(fit)\nmonthplot(time_series)\nseasonplot(time_series)"},{"path":"introduction-to-time-series.html","id":"stationary-time-series","chapter":"16 Introduction to Time Series","heading":"16.2 Stationary Time Series","text":"Stationarity one important concepts time series forecasting. stationary time series one whose properties depend time series observed. Thus, time series trends, seasonality, stationary — trend seasonality affect value time series different times. hand, white noise series stationary — matter observe , look much point time.Differencing process computing differences consecutive observations.apply log transformations stabilise variance time series. Differencing can also help stabilising mean time series removing changes level time series, therefore eliminating (reducing) trend seasonality. also tests check stationarity time series.","code":""},{"path":"introduction-to-time-series.html","id":"holtwinters-model","chapter":"16 Introduction to Time Series","heading":"16.3 HoltWinters model,","text":"model three parameters alpha, beta gamma.alpha: refers “base value”. Higher alpha puts weight recent observations.alpha: refers “base value”. Higher alpha puts weight recent observations.beta: corresponds “trend value”. Higher beta means trend slope dependent recent trend slopes.beta: corresponds “trend value”. Higher beta means trend slope dependent recent trend slopes.gamma: weighs “seasonal component”. Higher gamma puts weighting recent seasonal cycles.gamma: weighs “seasonal component”. Higher gamma puts weighting recent seasonal cycles.mathematical formulation follows,$$\n{y}{t+h|t} = l_t + hb_t + s{t+h-m(k+1)} \\l_t = (y_t - s_{t-m}) + (1- )*(l_{t-1} + b_{t-1})\\b_t = ^* (l_t - l_{t-1}) + (1-^*)b_{t-1}\\s_t = (y_t - l_{t-1} - b_{t-1}) + (1-)s_{t-m}\n$$Let’s start simplest model, model forecast future value last value , .e.y[t+] = y[t], >=1 t last time step training period.$$_{T+} = y_T \\\n>=1\n$$Now can set beta non zero value can account trends, model forecast future value last value + trend.Finally, set three parameters, alpha, beta gamma non zero values account trend seasonality .Naturally, model performs better previous two models.","code":"\n# simple exponential - models level\nfit_1 <- HoltWinters(time_series, beta=FALSE, gamma=FALSE)\nf1 = forecast(fit_1, 12)\nplot(forecast(fit_1, 12))\n# double exponential - models level and trend\nfit_2 <- HoltWinters(time_series, gamma=FALSE)\nf2 = forecast(fit_2, 12)\nplot(forecast(fit_2, 12))\n# triple exponential - models level, trend, and seasonal components\nfit_3 <- HoltWinters(time_series)\nf3 = forecast(fit_3, 12)\nplot(forecast(fit_3, 12))\n# Automated forecasting using an exponential model\nfit_4 <- ets(time_series)\nf4 = forecast(fit_4, 12)\nplot(forecast(fit_4, 12))"},{"path":"introduction-to-time-series.html","id":"exponential-smooting","chapter":"16 Introduction to Time Series","heading":"16.4 Exponential Smooting","text":"model, assign weights past observations decrease exponentially go back time. model ensures give weights recent observations less less weights older observations$$\n{T+1|T} = y_T + (1-)* y{t-1} + (1-)^2 *y_{t-2} + …..\\{T+1|T} = {j = 0}{T-1}(1-)jy_{T-j} + (1-)^Tl_0\n$$","code":""},{"path":"introduction-to-time-series.html","id":"arima-model","chapter":"16 Introduction to Time Series","heading":"16.5 ARIMA model,","text":"ARIMA stands Auto Regressive Integrated Moving Average. generalization simpler AutoRegressive Moving Average adds notion integration. Let’s look term individually.AR: Autoregression. refers model predicts future values using past observations.: Integrated. use concept differencing raw observations (e.g. subtracting observation observation previous time step) order make time series stationary.MA: Moving Average. refers model uses dependency observation residual error moving average model applied lagged observations.need three parameters define model. use notation ARIMA(p,d,q).parameters ARIMA model defined follows:p: number lag observations included model, also called lag order.\nd: number times raw observations differenced, also called degree differencing.\nq: size moving average window, also called order moving average.seasonal effect , generally considered better use SARIMA (seasonal ARIMA) model increase order AR MA parts model.Now, difficult choose values p, q can use Autocorrelation function (ACF), Partial autocorrelation function (PACF) plots series determine order AR / MA terms.autocorrelation function (ACF) technique can use identify correlated values time series . ACF plots correlation coefficient lag. lag corresponds certain point time observe first value time series.Partial autocorrelation statistical measure captures correlation two variables controlling effects variables.Now, can use another implementation ARIMA R .e auto.arima() function uses variation Hyndman-Khandakar algorithm (Hyndman & Khandakar, 2008). combines unit root tests, minimisation AICc MLE obtain ARIMA model., can clearly see MAPE RMSE bar plots Auto ARIMA ETS performed best followed Holt Winter\nmodel non zero alpha, beta gamma.References,Hyndman, R.J., & Athanasopoulos, G. (2018) Forecasting: principles practice, 2nd edition, OTexts: Melbourne,\nAustralia.OTexts.com/fpp2.Hyndman, R.J., & Athanasopoulos, G. (2018) Forecasting: principles practice, 2nd edition, OTexts: Melbourne,\nAustralia.OTexts.com/fpp2.https://en.wikipedia.org/wiki/Time_serieshttps://en.wikipedia.org/wiki/Time_serieshttps://www.analyticsvidhya.com/blog/2018/02/time-series-forecasting-methods/https://www.analyticsvidhya.com/blog/2018/02/time-series-forecasting-methods/","code":"\n#using an ARIMA model\nfit_5 <- Arima(time_series, order = c(10, 1, 1))\nf5 = forecast(fit_5, 12)\nplot(forecast(fit_5, 12))\n#using an ARIMA model\nacf(time_series)\npacf(time_series)\n# Forecasting using an ARIMA model\nfit_6 <- auto.arima(time_series)\nf6 = forecast(fit_6, 12)\nplot(forecast(fit_6, 12))\nrmse_1 = accuracy(forecast(fit_1))[2]\nrmse_2 = accuracy(forecast(fit_2))[2]\nrmse_3 = accuracy(forecast(fit_3))[2]\nrmse_4 = accuracy(forecast(fit_4))[2]\nrmse_5 = accuracy(forecast(fit_5))[2]\nrmse_6 = accuracy(forecast(fit_6))[2]\nrmse = c(rmse_1, rmse_2, rmse_3, rmse_4, rmse_5, rmse_6)\n\nbarplot(rmse, main=\"RMSE Plot\", xlab=\"Models\", names.arg = c(\"HW_1\", \"HW_2\", \"HW_3\", \"ETS\", \"ARIMA\", \"Auto ARIMA\"))\nMAPE_1 = accuracy(forecast(fit_1))[5]\nMAPE_2 = accuracy(forecast(fit_2))[5]\nMAPE_3 = accuracy(forecast(fit_3))[5]\nMAPE_4 = accuracy(forecast(fit_4))[5]\nMAPE_5 = accuracy(forecast(fit_5))[5]\nMAPE_6 = accuracy(forecast(fit_6))[5]\nMAPE = c(MAPE_1, MAPE_2, MAPE_3, MAPE_4, MAPE_5, MAPE_6)\n\nbarplot(MAPE, main=\"MAPE Plot\", xlab=\"Models\", names.arg = c(\"HW_1\", \"HW_2\", \"HW_3\", \"ETS\", \"ARIMA\", \"Auto ARIMA\"))"},{"path":"vipul-edav-community-contribution-kickstart-r.html","id":"vipul-edav-community-contribution-kickstart-r","chapter":"17 Vipul EDAV Community Contribution Kickstart R","heading":"17 Vipul EDAV Community Contribution Kickstart R","text":"Vipul H HariharThis hand written notes specially made understanding basics Exploratory Data Analysis R. material summation Professor Joyce’s teachings classroom lectures R documentation files.\ntried write neatly clearly reader might benefit notes easily, hope easily understandable legible.\ntopics involved notes follows:\n1) Introduction\n2) R Programming EDAV\n3) Fun Facts R\n4) Features R makes perfect “EDAV”\n5) Application R Programming\n6) Understanding Basic Data Types R\n7a) Handy Functions\n7b) Vector\n7c) List\n7d) Factors - IMPORTANT\n7e) Data Frame\n7f) Missing Values\n8) Functions R\n9) Data Science Process (Visual Description)\n10) visulizations use EDAV?Please find link submitted file:\nhttps://github.com/virslaan/EDAV_Community_Contribution/blob/main/Community%20Contribution%202.pdf","code":""},{"path":"caret---a-machine-learning-package-turotial.html","id":"caret---a-machine-learning-package-turotial","chapter":"18 caret - a machine learning package turotial","heading":"18 caret - a machine learning package turotial","text":"Jiang Zhu Xuechun BaiMotivation: motivation community contribution project introduce classmates package R can easily implement machine learning algorithms lines code. python users commonly use package scikit-learn deploy machine laerning models, strived find similar R package similar functions. finding resources online, discovered caret great package neatly defined functions create easy-use interface machine learning algorithms, time, abundant machine learning models preprocessing model selection functions. However, found great tutorial easy--read API’s caret. Therefore, markdown file, illustrate use caret package example-based manner. eliminate mathematics behind model much possible focus implementation algorithm R. also demonstrate results experiments caret model datasets visualization tools.","code":"\nlibrary(caret)\nlibrary(tidyverse)\nlibrary(readr)\nlibrary(kernlab)\nlibrary(earth)\nlibrary(RANN)"},{"path":"caret---a-machine-learning-package-turotial.html","id":"introduction-and-description-of-dataset","chapter":"18 caret - a machine learning package turotial","heading":"18.1 Introduction and description of dataset","text":"tutorial, use data frame Orange Juice includes two orange juice brands, Citrus Hill (CH) Minute Maid (MM) selling information according brand. URL dataset : https://raw.githubusercontent.com/selva86/datasets/master/orange_juice_withmissing.csv, contains 18 cloumns 1070 rows.first column “Purchase” introduces brand orange juice. data contains weekly amount purchase two brands (WeekofPurchase) store id (StoreID) orange juice sold, price orange juice according brand CH (PriceCH) MM (PriceMM). data also includes discount (“DiscCH” “DiscMM”), sale price(“SalePriceCH” “SalePriceMM”) useful information two orange juice brands. goal tutorial predict customers’ preference two brands choosing buy orange juice, focusing “Purchase” now.Now let us firstly import data, state briefly summary data:","code":"\n# Import dataset\ndf <- read.csv('https://raw.githubusercontent.com/selva86/datasets/master/orange_juice_withmissing.csv')\n\n# Structure of the dataframe\nhead(df)##   Purchase WeekofPurchase StoreID PriceCH PriceMM DiscCH DiscMM SpecialCH\n## 1       CH            237       1    1.75    1.99   0.00    0.0         0\n## 2       CH            239       1    1.75    1.99   0.00    0.3         0\n## 3       CH            245       1    1.86    2.09   0.17    0.0         0\n## 4       MM            227       1    1.69    1.69   0.00    0.0         0\n## 5       CH            228       7    1.69    1.69   0.00    0.0         0\n## 6       CH            230       7    1.69    1.99   0.00    0.0         0\n##   SpecialMM  LoyalCH SalePriceMM SalePriceCH PriceDiff Store7 PctDiscMM\n## 1         0 0.500000        1.99        1.75      0.24     No  0.000000\n## 2         1 0.600000        1.69        1.75     -0.06     No  0.150754\n## 3         0 0.680000        2.09        1.69      0.40     No  0.000000\n## 4         0 0.400000        1.69        1.69      0.00     No  0.000000\n## 5         0 0.956535        1.69        1.69      0.00    Yes  0.000000\n## 6         1 0.965228        1.99        1.69      0.30    Yes  0.000000\n##   PctDiscCH ListPriceDiff STORE\n## 1  0.000000          0.24     1\n## 2  0.000000          0.24     1\n## 3  0.091398          0.23     1\n## 4  0.000000          0.00     1\n## 5  0.000000          0.00     0\n## 6  0.000000          0.30     0"},{"path":"caret---a-machine-learning-package-turotial.html","id":"pre-processing","chapter":"18 caret - a machine learning package turotial","heading":"18.2 Pre-processing","text":"importing dataset, need separate data training testing sets. training set used examples machine learning models learn generalize, test set used evaluate preformance models. can achieve using createDataPartition() caret package.y specifies column partition datay specifies column partition datatimes specifies many times partition nthe datatimes specifies many times partition nthe datap specifies proportion split datap specifies proportion split datalist boolean indicating whether return listlist boolean indicating whether return listHere set 70% training set 30% testing set.next step clear refill missing values NAs dataset. several ways trasnformation fill missing value mean, mode simply delete row missing values. Using caret package, can apply NAs practical values, predict missing values values listed dataset. way caret package can use preProcess() function make k-Nearest Neighbors use training set.data specifies dataframe want apply preprocessingdata specifies dataframe want apply preprocessingmethod string specifies method preprocessingmethod string specifies method preprocessingThen get Nearest model perdicting missing values. , use predict() fill missing values prediction.result, can see prediction model centered 16 variables, ignored 2 created 825 samples 18 variables. importing NAs “predict” function, check missing values remaining dataset using angNA() function. result “FALSE” shows missing values replaced.Another work caret package can transform categorical variables one-hot vectors numerical representation categorical variables. one-hot vector constructed follows: suppose \\(n\\) classes categorical variables. represent class \\(\\) \\(n\\)-dimensional vector \\(o_i\\) \\(o_i[]=1\\) \\(o_i[j]=0\\) \\(j\\neq \\)can acheive one-hot transformation using dummyVars()formula repersents way inpute dataformula repersents way inpute datadata represents dataframedata represents dataframeAs can see, Store7.Store7.Yes represent one-hot encoding. actually apply dataset caret supports factor labels.","code":"\ncreateDataPartition(\n  y,\n  times,\n  p,\n  list\n)\n#set random seed to produce the same result\nset.seed(1)\n\n#use createDataPartition() function to get row of training. The input variable here is \"Purchase\" in df, and p = .7 implies the percentage of dataset we want to take. Here we set the training with 70%, so we let p equals to 0.7. Using List = F we are able to prevent the result as being a list instead of a dataframe.\nrowOfTrain <- createDataPartition(df$Purchase, p=0.7, list=FALSE)\n\n# create datasets for training and testing\ntrain_df <- df[rowOfTrain,]\ntest_df <- df[-rowOfTrain,]\npreProcess(\n  data,\n  method\n)\n# Create k-Nearest with training data using method = 'knnImpute'\nknn_inputer <- preProcess(train_df, method='knnImpute')\nknn_inputer## Created from 727 samples and 18 variables\n## \n## Pre-processing:\n##   - centered (16)\n##   - ignored (2)\n##   - 5 nearest neighbor imputation (16)\n##   - scaled (16)\n# Import the prediction into missing values using function predict()\ntrain_df <- predict(knn_inputer, newdata = train_df)\n\n#Check the NAs in the training dataset\nanyNA(train_df)## [1] FALSE\ndummyVars(\n  formula,\n  data\n)\n# define the one-hot encoding object\none_hot_encoder <- dummyVars(Purchase~., train_df)\n\n# results after applying one-hot encoding\nhead(data.frame(predict(one_hot_encoder, train_df)))##   WeekofPurchase   StoreID     PriceCH     PriceMM     DiscCH     DiscMM\n## 1     -1.1406992 -1.267157 -1.16230500 -0.70167093 -0.4532457 -0.5650681\n## 2     -1.0116414 -1.267157 -1.16230500 -0.70167093 -0.4532457  0.8702789\n## 3     -0.6244679 -1.267157 -0.08909746  0.03485153  0.9895487 -0.5650681\n## 4     -1.7859884 -1.267157 -1.74769093 -2.91123832 -0.4532457 -0.5650681\n## 5     -1.7214595  1.327114 -1.74769093 -2.91123832 -0.4532457 -0.5650681\n## 6     -1.5924017  1.327114 -1.74769093 -0.70167093 -0.4532457 -0.5650681\n##    SpecialCH  SpecialMM     LoyalCH SalePriceMM SalePriceCH  PriceDiff Store7No\n## 1 -0.4101973 -0.4461947 -0.24490422  0.08791265  -0.4606397  0.3276049        1\n## 2 -0.4101973  2.2381700  0.08369962 -1.10522569  -0.4606397 -0.7838676        1\n## 3 -0.4101973 -0.4461947  0.34658269  0.48562543  -0.8807610  0.9203903        1\n## 4 -0.4101973 -0.4461947 -0.57350806 -1.10522569  -0.8807610 -0.5615731        1\n## 5 -0.4101973 -0.4461947  1.25528731 -1.10522569  -0.8807610 -0.5615731        0\n## 6 -0.4101973  2.2381700  1.28385285  0.08791265  -0.8807610  0.5498994        0\n##   Store7Yes  PctDiscMM PctDiscCH ListPriceDiff      STORE\n## 1         0 -0.5718037 -0.449862     0.2132873 -0.4419062\n## 2         0  0.9396462 -0.449862     0.2132873 -0.4419062\n## 3         0 -0.5718037  1.016257     0.1218262 -0.4419062\n## 4         0 -0.5718037 -0.449862    -1.9817791 -0.4419062\n## 5         1 -0.5718037 -0.449862    -1.9817791 -1.1401926\n## 6         1 -0.5718037 -0.449862     0.7620539 -1.1401926"},{"path":"caret---a-machine-learning-package-turotial.html","id":"model-training","chapter":"18 caret - a machine learning package turotial","heading":"18.3 Model training","text":"machine learning models defined trained caret train() function. core function masks tedious detail machine learning algorithms provide convenient concise interface. train() function versions different parameters:formula specifies way want construct modelformula specifies way want construct modeldata specifies training data want use construct labeldata specifies training data want use construct labelmethod string specifiying model want apply datamethod string specifiying model want apply dataThe first version train function recommended data single dataframe. another word, column represents feature \\(x_i\\) one column represents \\(y\\). second version train function recommended \\(x\\) \\(y\\) separate dataframes. following code, use first version train() prediction.method parameter can specified conveniently string. far 238 models supported caret can found https://topepo.github.io/caret/available-models.html. tutorial, illustrate apply several common classification algorithms dataset.One amazing feature caret almost methods, automatically finds best hyperparameters user.","code":"\n# first version of train\nmodel <- train(\n  formula,\n  data,\n  method,\n)\n\n# second version of train\nmodel <- train(\n  x,\n  y,\n  method,\n)"},{"path":"caret---a-machine-learning-package-turotial.html","id":"k-nearest-neighbors","chapter":"18 caret - a machine learning package turotial","heading":"18.3.1 K-nearest neighbors","text":"k-nearest neighbor intuitive classification algorithm. Given positive \\(k\\) data \\(x\\), k-neigherest neighbor finds k nearest neighbors training set based euclidian distance\n\\[dist(x,x')=\\sqrt{(x_1-x'_1)^2+(x_2-x'_2)^2+\\cdots + (x_n-x'_n)^2}\\]\nclassifies \\(x\\) class contains nearst neighbors.implement algorithm, specify parameter method=knn","code":"\n# training the knn model\nknn_model <- train(\n  Purchase ~.,\n  train_df,\n  method = \"knn\"\n)\n\nknn_model## k-Nearest Neighbors \n## \n## 750 samples\n##  17 predictor\n##   2 classes: 'CH', 'MM' \n## \n## No pre-processing\n## Resampling: Bootstrapped (25 reps) \n## Summary of sample sizes: 750, 750, 750, 750, 750, 750, ... \n## Resampling results across tuning parameters:\n## \n##   k  Accuracy   Kappa    \n##   5  0.7430519  0.4561180\n##   7  0.7506511  0.4701385\n##   9  0.7522812  0.4717872\n## \n## Accuracy was used to select the optimal model using the largest value.\n## The final value used for the model was k = 9."},{"path":"caret---a-machine-learning-package-turotial.html","id":"logistics-regression","chapter":"18 caret - a machine learning package turotial","heading":"18.3.2 Logistics Regression","text":"logistics regression linear model binary classification, another word, given features \\(x_1,x_2,\\cdots,x_n\\), want predict \\(y=\\{0,1\\}\\). model can described equation\n\\[\\hat{y}=\\sigma(w_1\\cdot x_1+w_2\\cdot x_2+\\cdots+w_n\\cdot x_n)\\]\n\\(\\sigma\\) function defined \n\\[\\sigma(x)=\\frac{1}{1+e^{-x}}\\]hope logistics regression algorithm can find optimal \\(w_1,\\cdots,w_n\\) \\(\\hat{y}\\) closed true label \\(y\\) possible.implement algorithm, specify parameter method=\"glm\":","code":"\n# training the logistics regression model\nlr_model <- train(\n  Purchase ~.,\n  train_df,\n  method = \"glm\"\n)\n\nlr_model## Generalized Linear Model \n## \n## 750 samples\n##  17 predictor\n##   2 classes: 'CH', 'MM' \n## \n## No pre-processing\n## Resampling: Bootstrapped (25 reps) \n## Summary of sample sizes: 750, 750, 750, 750, 750, 750, ... \n## Resampling results:\n## \n##   Accuracy   Kappa    \n##   0.8219706  0.6207552"},{"path":"caret---a-machine-learning-package-turotial.html","id":"support-vector-machine","chapter":"18 caret - a machine learning package turotial","heading":"18.3.3 Support vector machine","text":"support vector machine hyperplane separates input space two subspaces. case \\(x_1\\) \\(x_2\\), want draw line saparates cartesian plane. criteria choosing plane one maximizes distance closest data points classes. gives rise optimization objective\n\\[\\max_{w,b}\\frac{1}{\\vert\\vert w\\vert\\vert_2}\\min_{x_i\\D}\\vert w^Tx_i+b\\vert,\\quad \\text{ s.t. }\\forall ,\\quad y_i(w^Tx_i+b)\\ge 0 \\]\nimplement algorithm, specify parameter method=\"svmRadial\"","code":"\n# training the support vector machine model, the library kernlab is needed to perform kernal transformation\nsvm_model <- train(\n  Purchase ~.,\n  train_df,\n  method = \"svmRadial\"\n)\n\nsvm_model## Support Vector Machines with Radial Basis Function Kernel \n## \n## 750 samples\n##  17 predictor\n##   2 classes: 'CH', 'MM' \n## \n## No pre-processing\n## Resampling: Bootstrapped (25 reps) \n## Summary of sample sizes: 750, 750, 750, 750, 750, 750, ... \n## Resampling results across tuning parameters:\n## \n##   C     Accuracy   Kappa    \n##   0.25  0.8126657  0.5977679\n##   0.50  0.8143211  0.6016239\n##   1.00  0.8123949  0.5975775\n## \n## Tuning parameter 'sigma' was held constant at a value of 0.05934766\n## Accuracy was used to select the optimal model using the largest value.\n## The final values used for the model were sigma = 0.05934766 and C = 0.5."},{"path":"caret---a-machine-learning-package-turotial.html","id":"random-forest","chapter":"18 caret - a machine learning package turotial","heading":"18.3.4 Random Forest","text":"decision tree algorithm learns predict value target variable learning simple decision rules inferred data features. process constructing tree first select root node \\(x_i\\) decision boundry \\(b_i\\). recirsively select children nodes corresponding decision threshold \\(b\\) maximizes information gain. Since different initialization produces different results, random forest aims create multiple trees different initialization collectively ageraging result produced individual decision treesTo implement algorithm, specify parameter method=\"earch\"","code":"\n# training the random forest model, the library earth is needed\nrf_model <- train(\n  Purchase ~.,\n  train_df,\n  method = \"earth\"\n)\n\nrf_model## Multivariate Adaptive Regression Spline \n## \n## 750 samples\n##  17 predictor\n##   2 classes: 'CH', 'MM' \n## \n## No pre-processing\n## Resampling: Bootstrapped (25 reps) \n## Summary of sample sizes: 750, 750, 750, 750, 750, 750, ... \n## Resampling results across tuning parameters:\n## \n##   nprune  Accuracy   Kappa    \n##    2      0.7958702  0.5634097\n##   12      0.8095924  0.5959591\n##   23      0.8081299  0.5933340\n## \n## Tuning parameter 'degree' was held constant at a value of 1\n## Accuracy was used to select the optimal model using the largest value.\n## The final values used for the model were nprune = 12 and degree = 1."},{"path":"caret---a-machine-learning-package-turotial.html","id":"model-prediction-and-evaluation","chapter":"18 caret - a machine learning package turotial","heading":"18.4 Model prediction and evaluation","text":"successfully train models, time evaluate test set validate good model. key function achieve predict() function:evaluate models trained , can call predict() function models:can compute confution matrix model:Confusion matrix knn model:Confusion matrix logistics regression model:Confusion matrix svm model:Confusion matrix random forest model:caret package also provides convenient function resamples() compare metric models.can see random forest best performance dataset.","code":"\npredict(\n  object,\n  newdata\n)\n# inpute the test data first\ntest_df <- predict(knn_inputer, test_df)\nknn_cm <- confusionMatrix(reference = factor(test_df$Purchase), data = predict(knn_model, test_df), mode='everything', positive='MM')\n\nknn_cm$table##           Reference\n## Prediction  CH  MM\n##         CH 169  30\n##         MM  26  95\nggplot(data = data.frame(knn_cm$table), aes(x=Reference, y=Prediction))+\n  geom_tile(aes(fill=Freq)) +\n  scale_fill_gradient2(low=\"light blue\", high=\"blue\") +\n  geom_text(aes(label=Freq)) +\n  ggtitle(\"Confution matrix for KNN model\")\nlr_cm <- confusionMatrix(reference = factor(test_df$Purchase), data = predict(lr_model, test_df), mode='everything', positive='MM')\n\nlr_cm$table##           Reference\n## Prediction  CH  MM\n##         CH 164  20\n##         MM  31 105\nggplot(data = data.frame(lr_cm$table), aes(x=Reference, y=Prediction))+\n  geom_tile(aes(fill=Freq)) +\n  scale_fill_gradient2(low=\"light blue\", high=\"blue\") +\n  geom_text(aes(label=Freq)) +\n  ggtitle(\"Confution matrix for Logistics Regression model\")\nsvm_cm <- confusionMatrix(reference = factor(test_df$Purchase), data = predict(svm_model, test_df), mode='everything', positive='MM')\n\nsvm_cm$table##           Reference\n## Prediction  CH  MM\n##         CH 170  27\n##         MM  25  98\nggplot(data = data.frame(svm_cm$table), aes(x=Reference, y=Prediction))+\n  geom_tile(aes(fill=Freq)) +\n  scale_fill_gradient2(low=\"light blue\", high=\"blue\") +\n  geom_text(aes(label=Freq)) +\n  ggtitle(\"Confution matrix for SVM model\")\nrf_cm <- confusionMatrix(reference = factor(test_df$Purchase), data = predict(svm_model, test_df), mode='everything', positive='MM')\n\nrf_cm$table##           Reference\n## Prediction  CH  MM\n##         CH 170  27\n##         MM  25  98\nggplot(data = data.frame(rf_cm$table), aes(x=Reference, y=Prediction))+\n  geom_tile(aes(fill=Freq)) +\n  scale_fill_gradient2(low=\"light blue\", high=\"blue\") +\n  geom_text(aes(label=Freq)) +\n  ggtitle(\"Confution matrix for random forest model\")\nmodel_list <- list(KNN = knn_model, LR = lr_model, SVM = svm_model, RF = rf_model)\n\nmodels_compare <- resamples(model_list)\n\nsummary(models_compare)## \n## Call:\n## summary.resamples(object = models_compare)\n## \n## Models: KNN, LR, SVM, RF \n## Number of resamples: 25 \n## \n## Accuracy \n##          Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's\n## KNN 0.7025090 0.7343173 0.7491166 0.7522812 0.7703180 0.8197880    0\n## LR  0.7909408 0.8118081 0.8201439 0.8219706 0.8297101 0.8763636    0\n## SVM 0.7620818 0.8013937 0.8172043 0.8143211 0.8239700 0.8525180    0\n## RF  0.7750000 0.7906137 0.8058608 0.8095924 0.8230769 0.8625954    0\n## \n## Kappa \n##          Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's\n## KNN 0.3824306 0.4294194 0.4604863 0.4717872 0.5098018 0.6148838    0\n## LR  0.5476278 0.5914299 0.6196513 0.6207552 0.6368782 0.7459377    0\n## SVM 0.4965787 0.5830797 0.6013610 0.6016239 0.6178862 0.6848073    0\n## RF  0.5198789 0.5579644 0.5906583 0.5959591 0.6214686 0.7077705    0"},{"path":"caret---a-machine-learning-package-turotial.html","id":"reference","chapter":"18 caret - a machine learning package turotial","heading":"18.5 Reference","text":"Caret Package – Practical Guide Machine Learning R - Selva Prabhakaran:\nhttps://www.machinelearningplus.com/machine-learning/caret-package/#61howtotrainthemodelandinterprettheresults?basic tutorial caret: machine learning package R - Rebecca Barter:\nhttps://www.rebeccabarter.com/blog/2017-11-17-caret_tutorial/caret Package - Max Kuhn:\nhttps://topepo.github.io/caret/index.html","code":""},{"path":"data-science-product-cycles-in-enterprises.html","id":"data-science-product-cycles-in-enterprises","chapter":"19 Data Science Product Cycles in Enterprises","heading":"19 Data Science Product Cycles in Enterprises","text":"Leo Du Hao LiAs Data Science students, typically work carefully prepared data problem sets professors designed lots effort guide students achieve learning objectives can mostly clearly measured within reasonable timeline. However, reality, almost never seamless achieve results. delivered Zoom talk Nov. 1st share unique perspectives different stages typical development life cycle Data Science powered products large enterprises, different teams involved, technologies often used, visualization tools techniques leveraged different stages.Due policies, share materials publicly. slide deck, well recording session available Courseworks submission, students taking EDAV Fall 2021 upon request.","code":""},{"path":"github-initial-setup.html","id":"github-initial-setup","chapter":"20 Github initial setup","heading":"20 Github initial setup","text":"Joyce Robbins","code":""},{"path":"github-initial-setup.html","id":"create-new-repo","chapter":"20 Github initial setup","heading":"20.1 Create new repo","text":"Create new repository copying template: http://www.github.com/jtr13/cctemplate following instructions README.","code":""},{"path":"github-initial-setup.html","id":"pages-in-repo-settings","chapter":"20 Github initial setup","heading":"20.2 Pages in repo settings","text":"Change source gh-pagesMay trigger GHA get work","code":""},{"path":"github-initial-setup.html","id":"add-packages-to-description-file","chapter":"20 Github initial setup","heading":"20.3 Add packages to DESCRIPTION file","text":"Need better process…Downloaded submissions CourseWorksCreate DESCRIPTION file. Add add dependencies projthis::proj_update_deps()https://twitter.com/ijlyttle/status/1370776366585614342Add Imports real DESCRIPTION file.Found problematic packages looking reverse dependencies packages failed install:devtools::revdep()Also used pak::pkg_deps_tree()Problems:magickrJava dependency qdap","code":""},{"path":"tutorial-for-pull-request-mergers.html","id":"tutorial-for-pull-request-mergers","chapter":"21 Tutorial for pull request mergers","heading":"21 Tutorial for pull request mergers","text":"","code":""},{"path":"tutorial-for-pull-request-mergers.html","id":"general","chapter":"21 Tutorial for pull request mergers","heading":"21.1 General","text":"following checklist steps perform merging pull request. point, ’re sure , request review one PR leaders.","code":""},{"path":"tutorial-for-pull-request-mergers.html","id":"check-branch","chapter":"21 Tutorial for pull request mergers","heading":"21.2 Check branch","text":"PR submitted non-main branch.PR submitted main branch, provide instructions fix problem:Close PR.Close PR.Follow instructions forgetting branch committed pushed GitHub: https://edav.info/github#fixing-mistakesFollow instructions forgetting branch committed pushed GitHub: https://edav.info/github#fixing-mistakesIf trouble 2., delete local folder project, delete fork GitHub, start .trouble 2., delete local folder project, delete fork GitHub, start .Open new PR.Open new PR.","code":""},{"path":"tutorial-for-pull-request-mergers.html","id":"examine-files-that-were-added-or-modified","chapter":"21 Tutorial for pull request mergers","heading":"21.3 Examine files that were added or modified","text":"ONE .Rmd file.ONE .Rmd file.additional resources resources/<project_name>/ folder.additional resources resources/<project_name>/ folder.files root directory besides .Rmd file.files root directory besides .Rmd file.","code":""},{"path":"tutorial-for-pull-request-mergers.html","id":"check-.rmd-filename","chapter":"21 Tutorial for pull request mergers","heading":"21.4 Check .Rmd filename","text":".Rmd filename words joined underscores, white space. (Update: need branch name.).Rmd filename can contain lowercase letters. (Otherwise filenames sort nicely repo home page.)","code":""},{"path":"tutorial-for-pull-request-mergers.html","id":"check-.rmd-file-contents","chapter":"21 Tutorial for pull request mergers","heading":"21.5 Check .Rmd file contents","text":"file contain YAML header --- line.first line start single hashtag #, followed single whitespace, title.second line blank, followed author name(s).additional single hashtag headers chapter. (, new chapters created.)hashtag headers followed numbers since hashtags create numbered subheadings. Correct: ## Subheading. Incorrect: ## 3. Subheading.file contains setup chunk .Rmd file, contain setup label. (bookdown render fail duplicate chunk labels.)\n.e. use {r, include=FALSE} instead {r setup, include=FALSE}.\nSee sample .RmdLinks internal files must contain resources/<project_name>/ path, : ![Test Photo](resources/sample_project/election.jpg)file contain install.packages(), write functions, setwd(), getwd().’s anything else looks odd ’re sure, assign jtr13 review explain issue.","code":""},{"path":"tutorial-for-pull-request-mergers.html","id":"request-changes","chapter":"21 Tutorial for pull request mergers","heading":"21.6 Request changes","text":"problems checks listed , explain pull request merged request changes following steps:, add changes requested label pull request.job pull request done now. contributors fix requests, review either move forward merge explain changes still need made.","code":""},{"path":"tutorial-for-pull-request-mergers.html","id":"merge-the-pull-request","chapter":"21 Tutorial for pull request mergers","heading":"21.7 Merge the pull request","text":"good go, ’s time merge pull request. several steps.","code":""},{"path":"tutorial-for-pull-request-mergers.html","id":"steps-to-merge-the-pr","chapter":"21 Tutorial for pull request mergers","heading":"21.7.1 Steps to Merge the PR","text":"Go main branch project (jtr13/cc21fall1) open _bookdown.yml fileGo main branch project (jtr13/cc21fall1) open _bookdown.yml fileCopy entire rmd_files section. look something like \nrmd_files: [ 'index.Rmd', # must first chapter 'assignment.Rmd', ...., ...., ]Copy entire rmd_files section. look something like \nrmd_files: [ 'index.Rmd', # must first chapter 'assignment.Rmd', ...., ...., ]Open branch submitted PR following steps:\naccess PR branch:\n\nMake sure PR branch checking PR branch name shown (main):\nOpen branch submitted PR following steps:access PR branch:Make sure PR branch checking PR branch name shown (main):Remove rmd_files: [] section paste one copied main branch project.Remove rmd_files: [] section paste one copied main branch project.Add name new file single quotes followed comma labelled section (eg. Cheatsheets, Tutorials etc).Add name new file single quotes followed comma labelled section (eg. Cheatsheets, Tutorials etc).Save edited version.Save edited version.Come back PR.Come back PR.Merge PR.Merge PR.Click Actions tabs check whether build successful (successful build green dot front actions). PLEASE NOTE actions take complete (approximately 5-6 mins depending number files rendered), might need wait time finally check whether build successful .Click Actions tabs check whether build successful (successful build green dot front actions). PLEASE NOTE actions take complete (approximately 5-6 mins depending number files rendered), might need wait time finally check whether build successful .case build fail able understand rectify please tag one PR Assigners can review . PLEASE revert merge create new branches workflow.case build fail able understand rectify please tag one PR Assigners can review . PLEASE revert merge create new branches workflow.","code":""},{"path":"tutorial-for-pull-request-mergers.html","id":"pr-leaders-only-add-part-names-to-.rmd-for-every-first-article-in-part","chapter":"21 Tutorial for pull request mergers","heading":"21.7.2 PR Leaders only: Add part names to .Rmd for every first article in part","text":"adding first chapter PART.every first article part, add chapter name top .Rmd file, propose changes. example like .\n","code":""},{"path":"tutorial-for-pull-request-mergers.html","id":"merge-pr-and-leave-a-comment","chapter":"21 Tutorial for pull request mergers","heading":"21.7.3 Merge PR and leave a comment","text":"Now comes final step.develop experience confident things correctly, assign another PR merger review work:@aye21874 (Ayush), @clarissarjtai (Clarissa), @ejosied (Josie), @hwelinkim (Hyo Won), @ivanye2509 (Xin)\n@kfijan (Katharina), @ktsht (Alex), @mtz2110 (Maxwell), @s10singh97 (Shashwat), @ShiyuWang88 (Shiyu), @Shruti-Kaushal (Shruti), @verlocks (Zheyu)(Please fix names mistakes aren’t names go … don’t need submit pull request, just edit file commit main branch.)Go back conversation tab pull requests page, example:https://github.com/jtr13/cc20/pull/23#issuecomment-728506101Leave comments congratulations 🎉 (type :tada:) click green button merge.\n","code":""},{"path":"tutorial-for-pull-request-mergers.html","id":"check-updated-version","chapter":"21 Tutorial for pull request mergers","heading":"21.7.4 Check updated version","text":"successful merge means addition file files added project merge conflicts. mean book render deploy GitHub pages without issues. merge, take 5-10 minutes GitHub Actions render book deploy updated version. ’s problem notified email address . words, job done. However ’re interested, can check progress clicking Actions top repo.","code":""}]
